{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"figs/LogoUFSCar.jpg\" alt=\"Logo UFScar\" width=\"110\" align=\"left\"/>  <br/> <center>Universidade Federal de São Carlos (UFSCar)<br/><font size=\"4\"> Departamento de Computação, campus Sorocaba</center></font>\n",
    "</p>\n",
    "\n",
    "<font size=\"4\"><center><b>Disciplina: Aprendizado de Máquina</b></center></font>\n",
    "  \n",
    "<font size=\"3\"><center>Prof. Dr. Tiago A. Almeida</center></font>\n",
    "\n",
    "## <center>Projeto Final</center>\n",
    "\n",
    "**Aluno**: Eduardo Garcia do Nascimento\n",
    "\n",
    "**RA/CPF**: 22008732800\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Carregamento dos dados\n",
    "\n",
    "Nesta seção é feita a carga dos atributos em um dataframe só, ou seja, os três datasets são lidos e concatenados para que a redução de atributos leve em conta o que existe de melhor em todos eles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-63a92bd8a1c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mset1_dataset\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILES_DIRECTORY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mset2_dataset\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILES_DIRECTORY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mset3_dataset\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILES_DIRECTORY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set3.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILES_DIRECTORY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtest_dataset\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILES_DIRECTORY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2155\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1461\u001b[0m     \"\"\"\n\u001b[1;32m   1462\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Caminho dos arquivos\n",
    "FILES_DIRECTORY = \"data\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from scripts import utils\n",
    "\n",
    "if __name__ == '__main__':\n",
    "                       \n",
    "    # importa o arquivo e guarda em um dataframe do Pandas\n",
    "    set1_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set1.csv'), sep=',', low_memory=False)\n",
    "    set2_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set2.csv'), sep=',', low_memory=False) \n",
    "    set3_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set3.csv'), sep=',', low_memory=False)\n",
    "    train_dataset = pd.read_csv(os.path.join(FILES_DIRECTORY, 'train.csv'), sep=',')\n",
    "    test_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'test.csv'), sep=',')\n",
    "    \n",
    "    # Renomeia colunas concatenando o setX antes de fazer o merge para identificá-las posteriormente\n",
    "    cols = set1_dataset.columns\n",
    "    for col in cols:\n",
    "        set1_dataset = set1_dataset.rename(columns={col:'set1_'+col})\n",
    "        \n",
    "    cols = set2_dataset.columns\n",
    "    for col in cols:\n",
    "        set2_dataset = set2_dataset.rename(columns={col:'set2_'+col})\n",
    "    \n",
    "    cols = set3_dataset.columns\n",
    "    for col in cols:\n",
    "        set3_dataset = set3_dataset.rename(columns={col:'set3_'+col})\n",
    "\n",
    "    # Concatena os datasets em somente um dataset único\n",
    "    frames = [ set1_dataset, set2_dataset, set3_dataset ]\n",
    "    input_dataset = pd.concat(frames, axis=1)\n",
    "    \n",
    "    print('A base de dados inicial combinada tem %d amostras com %d atributos.' % (input_dataset.shape[0],\n",
    "                                                                                 input_dataset.shape[1]))\n",
    "    \n",
    "        \n",
    "   \n",
    "    utils.beep(1, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Pré-processamento e seleção de atributos\n",
    "\n",
    "Nesta seção são feitas limpezas da base de dados como:\n",
    "\n",
    "* Remoção de atributos sem variância;\n",
    "* Tratamento de outliers e dados nulos;\n",
    "* Seleção dos atributos que terão maior valor para o algoritmo de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove os atributos que são constantes e não oferecem nenhum valor aos algoritmos de classificação\n",
    "variance_mask = VarianceThreshold().fit(input_dataset).get_support()\n",
    "input_dataset = input_dataset.iloc[:,variance_mask]\n",
    "print('Atributos removidos por baixa variância: %d' % np.sum(~variance_mask))\n",
    "\n",
    "# Tratamento de outliers e entradas nulas\n",
    "df = input_dataset.copy()\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_mask = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))\n",
    "print(\"Número de outliers substituídos por valores nulos:\", np.sum(np.sum(outliers_mask)))\n",
    "df[outliers_mask] = np.nan\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "input_dataset.loc[:,:] = imp_mean.fit_transform(input_dataset)\n",
    "\n",
    "# Normalização dos dados entre 0 e 1\n",
    "input_dataset.loc[:,:] = MinMaxScaler().fit_transform(input_dataset)\n",
    "\n",
    "# Adiciona as classes junto ao dataset de atributos para fazer a seleção de atributos\n",
    "input_dataset['classe'] = np.nan\n",
    "input_dataset.loc[train_dataset['Id'].values,'classe'] = train_dataset['Class'].values\n",
    "\n",
    "# Salva uma cópia do dataset completo que será usada para predição de novas amostras.\n",
    "backup_dataset = input_dataset.copy()\n",
    "\n",
    "feature_cols = list(input_dataset.columns)\n",
    "feature_cols.remove('classe')\n",
    "\n",
    "# Seleciona os melhores atritubos para treinametno do algoritmo de classificação\n",
    "print('Selecionando melhores features....................................')\n",
    "selector = SelectKBest(f_classif, k=6).fit(\n",
    "    input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1), feature_cols].values,\n",
    "    input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1),'classe'].values)\n",
    "features_mask = selector.get_support()\n",
    "features_mask = np.append(features_mask, True)\n",
    "input_dataset = input_dataset.iloc[:,features_mask]\n",
    "\n",
    "print('\\n\\n\\n\\nBreve avaliação das primeiras amostras')\n",
    "display(input_dataset.head(10))\n",
    "\n",
    "print('Avaliação do descritivo do dataset que permite ter uma ideia mais realista dos dados')\n",
    "display(input_dataset.describe())\n",
    "\n",
    "utils.beep(1, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise exploratória\n",
    "\n",
    "Nesta seção são exibidas informações do resultado após os dados serem pré-processados e os atributos selecionados. Dentre as ferramentas para análise exploratória que serão utilizados estão:\n",
    "\n",
    "* Descritivo resumido da base.\n",
    "* Análises de covariância e correlação.\n",
    "* Matriz de disperação entre todos os atributos selecionados. \n",
    "* Diagramas de violino para visualização dos quartis e outliers como uma variação aos diagramas de caixa.\n",
    "* Gráfico de dispersão com a dimensionalidade reduzida a somente 2 atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Análise do balanceamento das classes\n",
    "print('Análise e visualização dos dados:')\n",
    "ax = sns.countplot(x='classe', data=input_dataset, label=\"Contagem\")\n",
    "\n",
    "N,U,P = input_dataset.classe.value_counts()\n",
    "print('Número de posts comuns:', N)\n",
    "print('Número de posts não reconhecidos (podem ser phishing ou não):', U)\n",
    "print('Número de posts phishing:', P)\n",
    "plt.show()\n",
    "\n",
    "# Análise da matriz scatter pra que entendamos a relação entre os atributos\n",
    "print('Análise da matriz de dispersão')\n",
    "sns.pairplot(input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)], hue='classe', height=3.5);\n",
    "plt.show()\n",
    "\n",
    "# matrizes de covariancia e correlação\n",
    "print('Análise das matrizes de covariância ')\n",
    "df_covariance = input_dataset.iloc[:,:-1].cov()\n",
    "df_correlation = input_dataset.iloc[:,:-1].corr()\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20, 8))\n",
    "plt.title('Covariância')\n",
    "sns.heatmap(df_covariance, annot=True, xticklabels=df_correlation.columns, yticklabels=df_correlation.columns, ax=ax1)\n",
    "plt.title('Correlação')\n",
    "sns.heatmap(df_correlation, annot=True, xticklabels=df_correlation.columns, yticklabels=df_correlation.columns, ax=ax2)\n",
    "plt.show()\n",
    "\n",
    "# Montagem do dataset de teste para envio para o Kaggle\n",
    "K = input_dataset.loc[test_dataset.iloc[:,:].values.T[0]]\n",
    "K = K.drop('classe', axis=1).values\n",
    "\n",
    "# Diagramas de caixa\n",
    "plt.figure(figsize=(20,10))\n",
    "data = pd.melt(input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)],\n",
    "               id_vars=\"classe\", var_name=\"features\", value_name='value')\n",
    "sns.boxplot(x='features', y='value', hue='classe', data=data)\n",
    "plt.show()\n",
    "\n",
    "# Diagramas de violino\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.violinplot(x='features', y='value', hue='classe', data=data, split=True, inner=\"quartile\")\n",
    "plt.show()\n",
    "\n",
    "# Separação de atributos e classe para \n",
    "utils.printPCA(input_dataset.loc[((input_dataset['classe'] == -1) | (input_dataset['classe'] == 1))].drop('classe', axis=1).values,\n",
    "               input_dataset.loc[((input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)), 'classe'].values)    \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(input_dataset.loc[((input_dataset['classe'] == -1) | (input_dataset['classe'] == 1))].drop('classe', axis=1).values)\n",
    "data = pd.DataFrame(data=projected, columns=['a', 'b'])\n",
    "data['classe'] = input_dataset.loc[((input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)), 'classe'].values\n",
    "\n",
    "if int(sns.__version__[2:4]) < 11:\n",
    "    print('Para uma melhor experiência, atualize a seaborn para a versão 11 ou superior')\n",
    "    sns.jointplot(data=data, x=\"a\", y=\"b\")\n",
    "else:\n",
    "    sns.jointplot(data=data, x=\"a\", y=\"b\", kind=\"kde\", palette='Spectral', hue='classe')\n",
    "\n",
    "utils.beep(1, 600)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Preparo dos dados e experimentos para encontrar os melhores hiperparâmetros\n",
    "\n",
    "Nesta seção os dados são separados em duas partes: treino e testes. Esta estratégia foi utilizada para posterior comparação com a validação de modelos utilizandos K-folds.\n",
    "Para encontrar os melhores hiperparâmetros foi utilizada a classe GridSearchCV e devido o seu alto custo computacional e não existência da necessidade de executá-las sempre, a sua chamada é condicionada às variáveis booleanas evaluate_svm_hiperparameters, evaluate_rfc_hiperparameters e evaluate_lrc_hiperparameters serem verdadeiras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scripts import utils\n",
    "\n",
    "X_total = input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)].drop('classe', axis=1).values\n",
    "y_total = input_dataset.loc[((input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)), 'classe'].values\n",
    "\n",
    "# Utilize as flags abaixo somente para avaliação dos hiperparâmetros pois elas demoram muito pra serem executadas\n",
    "evaluate_svm_hiperparameters = False # Busca os melhores parâmetros para as máquinas de vetores de suporte\n",
    "evaluate_rfc_hiperparameters = False # Busca os melhores parâmetros para as florestas aleatórias\n",
    "evaluate_knn_hiperparameters = False # Busca os melhores parâmetros para o KNN\n",
    "\n",
    "print('Removendo amostras outliers')\n",
    "X, y = utils.remove_outliers(X_total, y_total)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print('Separando a base em treino e teste')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "scores = ['balanced_accuracy', 'f1', 'roc_auc']\n",
    "\n",
    "if evaluate_svm_hiperparameters == True:\n",
    "    utils.find_best_svm(X,y, scores)\n",
    "    \n",
    "if evaluate_rfc_hiperparameters == True:    \n",
    "    utils.find_best_rfc(X, y, scores)\n",
    "        \n",
    "if evaluate_knn_hiperparameters == True:\n",
    "    utils.find_best_knn(X, y, scores)\n",
    "\n",
    "utils.beep(1, 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Experimento\n",
    "\n",
    "Nesta seção, o experimento deve ser conduzido, utilizando os protocolos experimentais ensinados no curso e executando os métodos inteligentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import  plot_confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "\n",
    "X_bal, y_bal = X, y\n",
    "X_train_bal, y_train_bal = X_train, y_train\n",
    "\n",
    "\n",
    "scoring=['roc_auc', 'f1', 'f1_micro', 'f1_macro', 'f1_weighted', 'accuracy', 'balanced_accuracy', 'precision', 'recall']\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "model_list = [\n",
    "    ['SVM poly', svm.SVC(kernel='poly', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "   # ['SVM linear', svm.SVC(kernel='linear', C=100, class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "   # ['SVM rbf', svm.SVC(kernel='rbf', C=1000, gamma=0.0001, class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "   # ['SVM rbf_ra', svm.SVC(kernel='rbf', C=10, gamma='scale', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "   # ['SVM rbf_ba', svm.SVC(kernel='rbf', C=400, gamma='scale', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "   # ['SVM rbf_f1', svm.SVC(kernel='rbf', C=600, gamma=0.001, class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "    ['Random Forest', RandomForestClassifier(max_depth=2, class_weight='balanced', random_state=1), 0],\n",
    "    ['Logistic Regression', LogisticRegression(random_state=1, class_weight='balanced', max_iter=15000), 0],\n",
    "    ['Multinomial NB', MultinomialNB(), 0],\n",
    "    ['KNN', KNeighborsClassifier(n_neighbors=1, leaf_size=4, weights='distance'), 0],\n",
    "   # ['AdaBoost', AdaBoostClassifier(n_estimators=100, random_state=0), 0],\n",
    "   # ['GradBoost', GradientBoostingClassifier(random_state=0), 0],\n",
    "   # ['KNN3', KNeighborsClassifier(n_neighbors=1, leaf_size=4, weights='distance'), 0],\n",
    "   # ['KNN8', KNeighborsClassifier(n_neighbors=70, leaf_size=1, weights='distance'), 0],\n",
    "]\n",
    "\n",
    "print(np.sum(y_train_bal==-1),np.sum(y_train_bal==1))\n",
    "print(np.sum(y_bal==-1),np.sum(y_bal==1))\n",
    "\n",
    "# Pré balanceamento dos dados utilizando a técnica de oversampling\n",
    "X_train_bal, y_train_bal = utils.balance_classes(X_train_bal, y_train_bal)\n",
    "X_bal, y_bal = utils.balance_classes(X, y)\n",
    "\n",
    "utils.find_best_knn(X_bal, y_bal, scores)\n",
    "\n",
    "print(np.sum(y_train_bal==-1),np.sum(y_train_bal==1))\n",
    "print(np.sum(y_bal==-1),np.sum(y_bal==1))\n",
    "\n",
    "semi_model = svm.SVC(kernel='poly', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1)\n",
    " # População dos dados não classificados com o melhor classificador encontrado com os resultados mais confiáveis\n",
    "if False:\n",
    "    samples_mask = (backup_dataset.classe!=-1)&(backup_dataset.classe!=1)&(backup_dataset.classe!=0)\n",
    "    samples = backup_dataset[samples_mask].iloc[:,features_mask].drop('classe', axis=1).values\n",
    "    X_semi, y_semi, proba_mask = utils.add_samples(semi_model,X_bal, y_bal, samples)\n",
    "\n",
    "    print('Número de amostras acrescentadas ao dataset de treinamento:', X_semi.shape[0], \n",
    "                                                                          np.sum(y_semi==-1), np.sum(y_semi==1))\n",
    "    X_train_bal = np.concatenate([X_train_bal, X_semi], axis=0)\n",
    "    y_train_bal = np.concatenate([y_train_bal, y_semi], axis=0)\n",
    "    X_bal = np.concatenate([X_bal, X_semi], axis=0)\n",
    "    y_bal = np.concatenate([y_bal, y_semi], axis=0)\n",
    "\n",
    "if False:\n",
    "    #X_train_bal, y_train_bal = utils.balance_classes(X_train_bal, y_train_bal)\n",
    "    #X_bal, y_bal = utils.balance_classes(X_bal, y_bal)\n",
    "    X_bal, y_bal = utils.downsample(X_bal, y_bal)\n",
    "    # População dos dados não classificados com o melhor classificador encontrado com os resultados mais confiáveis\n",
    "    samples = samples[~proba_mask]\n",
    "    X_semi, y_semi, proba_mask = utils.add_samples(semi_model, X_train_bal, y_train_bal, samples)\n",
    "    print('Número de amostras acrescentadas ao dataset de treinamento:', X_semi.shape[0], \n",
    "                                                                          np.sum(y_semi==-1), np.sum(y_semi==1))\n",
    "    X_train_bal = np.concatenate([X_train_bal, X_semi], axis=0)\n",
    "    y_train_bal = np.concatenate([y_train_bal, y_semi], axis=0)\n",
    "    X_bal = np.concatenate([X_bal, X_semi], axis=0)\n",
    "    y_bal = np.concatenate([y_bal, y_semi], axis=0)\n",
    "\n",
    "print(np.sum(y_train_bal==-1),np.sum(y_train_bal==1))\n",
    "print(np.sum(y_bal==-1),np.sum(y_bal==1))\n",
    "\n",
    "#X_train_bal, y_train_bal = utils.downsample(X_train_bal, y_train_bal)\n",
    "#X_bal, y_bal = utils.downsample(X_bal, y_bal)\n",
    "#X_train_bal, y_train_bal = utils.balance_classes(X_train_bal, y_train_bal)\n",
    "#X_bal, y_bal = utils.balance_classes(X, y)\n",
    "\n",
    "print(np.sum(y_train_bal==-1),np.sum(y_train_bal==1))\n",
    "print(np.sum(y_bal==-1),np.sum(y_bal==1))\n",
    "\n",
    "# Configura K folds estratificados, ou seja, mantendo as mesmas proporções entre classes\n",
    "cv = StratifiedKFold(n_splits=4, random_state=1, shuffle=True)\n",
    "\n",
    "for model in model_list:\n",
    "    print(model[0] + '------------------------------------------------------------------------------------')\n",
    "    y_pred = utils.evaluate_model(model[1], X_train_bal, y_train_bal, X_test, y_test)\n",
    "    \n",
    "    cv_results = cross_validate(model[1], X_bal, y_bal, scoring=scoring, return_train_score=True)\n",
    "\n",
    "    for cv_result in cv_results:\n",
    "        results.loc[model[0], cv_result] = np.mean(cv_results[cv_result])\n",
    "    \n",
    "    results.loc[model[0], 'TN'] = confusion_matrix(y_test, y_pred)[0][0]\n",
    "    results.loc[model[0], 'TP'] = confusion_matrix(y_test, y_pred)[1][1]\n",
    "\n",
    "\n",
    "utils.beep(1, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise dos Resultados\n",
    "\n",
    "Nesta seção, os resultados devem ser exibidos e comparados, através de tabelas e gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    \n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "cols_filter = ['fit_time', 'score_time', 'test_roc_auc', 'test_f1', \n",
    "               'test_f1_micro', 'test_f1_macro', 'test_f1_weighted', \n",
    "              'test_accuracy', 'test_balanced_accuracy', 'train_balanced_accuracy', \n",
    "              'test_precision', 'test_recall', 'TN', 'TP']\n",
    "\n",
    "results_highlited = results.loc[:,cols_filter].style.apply(highlight_max)\n",
    "\n",
    "\n",
    "\n",
    "display(results_highlited)\n",
    "utils.beep(3, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Criação do arquivo de sumissão no Kaggle\n",
    "\n",
    "Na etapa final, o arquivo submission.csv é criado para ser enviado ao Kaggle de acordo com os padrões pré-definidos pela proposta do desafio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_preference_score = 'SVM poly'\n",
    "\n",
    "for model in model_list:\n",
    "    if model[0]==submit_preference_score:\n",
    "        best_model = model[1]\n",
    "        best_model_name = model[0]\n",
    "\n",
    "print('Criando arquivo de submissão para o modelo: ' + submit_preference_score)    \n",
    "    \n",
    "    \n",
    "print('Printing model to submission.csv ###################################################################')\n",
    "clf= best_model.fit(X_bal, y_bal)\n",
    "y_pred_submission = clf.predict_proba(K)[:,1]\n",
    "result = np.zeros((K.shape[0],2))\n",
    "for i in range(K.shape[0]):\n",
    "    result[i][0] = test_dataset.iloc[:,:].values.T[0][i]\n",
    "    result[i][1] = y_pred_submission[i]\n",
    "resultdf = pd.DataFrame(data=result, columns=[\"Id\", \"Predicted\"])\n",
    "resultdf['Id'] = resultdf['Id'].astype(int)\n",
    "resultdf['Predicted'] = resultdf['Predicted'].round(decimals=5)\n",
    "resultdf.to_csv('submission.csv', index=False, float_format='%.5f')\n",
    "print('####################################################################################################')\n",
    "\n",
    "utils.beep(5, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
