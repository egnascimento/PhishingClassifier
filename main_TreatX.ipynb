{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"figs/LogoUFSCar.jpg\" alt=\"Logo UFScar\" width=\"110\" align=\"left\"/>  <br/> <center>Universidade Federal de São Carlos (UFSCar)<br/><font size=\"4\"> Departamento de Computação, campus Sorocaba</center></font>\n",
    "</p>\n",
    "\n",
    "<font size=\"4\"><center><b>Disciplina: Aprendizado de Máquina</b></center></font>\n",
    "  \n",
    "<font size=\"3\"><center>Prof. Dr. Tiago A. Almeida</center></font>\n",
    "\n",
    "## <center>Projeto Final</center>\n",
    "\n",
    "**Aluno**: Eduardo Garcia do Nascimento\n",
    "\n",
    "**RA/CPF**: 22008732800\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise exploratória\n",
    "\n",
    "Nesta seção, deve ser feita a leitura da base de dados e todas as análises necessárias para entendê-la melhor, tais como:\n",
    "* Significado de cada atributo\n",
    "* Medidas descritivas\n",
    "* Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados concatenados produzindo um total de 310 atributos\n",
      "O número de amostras com classificação válida é: 4310\n",
      "Dados de treinamento carregados com sucesso!\n",
      "Dados de teste carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Caminho dos arquivos\n",
    "FILES_DIRECTORY = \"data\"\n",
    "\n",
    "import numpy as np  # importa a biblioteca usada para trabalhar com vetores e matrizes\n",
    "import pandas as pd # importa a biblioteca usada para trabalhar com dataframes (dados em formato de tabela) e análise de dados\n",
    "import os # importa a biblioteca para tarefas relacionadas ao sistema operacional\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "                       \n",
    "    # importa o arquivo e guarda em um dataframe do Pandas\n",
    "    set1_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set1.csv'), sep=',', low_memory=False)\n",
    "    set2_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set2.csv'), sep=',', low_memory=False) \n",
    "    set3_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set3.csv'), sep=',', low_memory=False) \n",
    "\n",
    "    # Concatena os datasets em somente um\n",
    "    frames = [ set1_dataset,set2_dataset,set3_dataset ]\n",
    "    input_dataset = pd.concat(frames, axis=1)\n",
    "\n",
    "    # Concatena as classes junto ao dataframe de atributos\n",
    "    train_dataset = pd.read_csv(os.path.join(FILES_DIRECTORY, 'train.csv'), sep=',')\n",
    "    input_dataset['classe'] = np.nan\n",
    "    input_dataset.loc[train_dataset['Id'].values,'classe'] = train_dataset['Class'].values\n",
    "\n",
    "    print('Dados concatenados produzindo um total de %d atributos' % \n",
    "            input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)].shape[1])\n",
    "    \n",
    "    print('O número de amostras com classificação válida é: %d' % \n",
    "            input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)].shape[0])\n",
    "\n",
    "    print('Dados de treinamento carregados com sucesso!')\n",
    "\n",
    "    test_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'test.csv'), sep=',')\n",
    "\n",
    "    print('Dados de teste carregados com sucesso!')\n",
    "   \n",
    "    duration = 0.5  # seconds\n",
    "    freq = 440  # Hz\n",
    "    os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Pré-processamento\n",
    "\n",
    "Nesta seção, as funções da etapa de pré-processamento dos dados devem ser implementadas e aplicadas (se necessário)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preenchendo arrays a partir dos datasets de entrada com dados cuja a classe é definida\n",
      "X (60842, 306)\n",
      "y (60842, 1)\n",
      "Removendo atributos com baixa variância....................................\n",
      "Atributos removidos por baixa variância: 6\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5fe9af503fa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m#X = utils.remove_outliersandnan(X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mimputer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'median'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "import utils\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#df = pd.DataFrame(data=[[2,3,200],[4,3,4],[2,3,2],[2,2,2],[2,3,2]])\n",
    "#Q1 = df.quantile(0.25)\n",
    "#Q3 = df.quantile(0.75)\n",
    "#IQR = Q3 - Q1\n",
    "#mask = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))\n",
    "#print(mask)\n",
    "#df[mask] = np.nan\n",
    "#display(df.head(10))\n",
    "#break\n",
    "\n",
    "if True:\n",
    "    print('Preenchendo arrays a partir dos datasets de entrada com dados cuja a classe é definida')\n",
    "    X = input_dataset.drop('classe', axis=1).drop('Id', axis=1).values\n",
    "    y = input_dataset[['classe']].values\n",
    "    print('X',X.shape)\n",
    "    print('y',y.shape)\n",
    "\n",
    "    print('Removendo atributos com baixa variância....................................')\n",
    "    variance_mask = VarianceThreshold().fit(X).get_support()\n",
    "    X = X[:,variance_mask]\n",
    "    print('Atributos removidos por baixa variância: %d' % np.sum(~variance_mask))\n",
    "\n",
    "    #print('Removendo atributos com alta correlação....................................')\n",
    "    #X, corr_mask = utils.remove_correlated(X, 0.95)\n",
    "    #print('Atributos removido por alta correlação: %d' % len(corr_mask))\n",
    "\n",
    "    #print('Balanceando amostras das classes: Antes: -1=%d 1=%d' % (np.sum(y==-1), np.sum(y==1)))\n",
    "    #X = utils.remove_outliersandnan(X)\n",
    "    \n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    X.loc[:,:] = imputer.fit_transform(X)\n",
    "    Q1 = X.quantile(0.25)\n",
    "    Q3 = X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = (X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))\n",
    "    print('Outliers encontrados e removidos: %d' % np.count_nonzero(mask == True))\n",
    "    X.loc[:,:] = np.where((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR)), np.nan, X)\n",
    "    display(X.head(10))\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    X.loc[:,:] = imputer.fit_transform(X)\n",
    "    display(X.head(10))\n",
    "    \n",
    "    print('Balanceando amostras das classes: Depois: -1=%d 1=%d' % (np.sum(y==-1), np.sum(y==1)))\n",
    "\n",
    "    #print('Normalizing scale between 0 and 1..................................')\n",
    "    X = pd.DataFrame(data=MinMaxScaler().fit_transform(X))\n",
    "    K = X.loc[test_dataset.iloc[:,:].values.T[0]].values\n",
    "\n",
    "    #Xsemi = input_dataset.loc[(input_dataset['classe'] != -1) & (input_dataset['classe'] != 1)].drop('classe', axis=1).drop('Id', axis=1).values\n",
    "    X = X.iloc[((y == -1).values) | ((y == 1).values)].values\n",
    "    y = y.loc[((y == -1).values) | ((y == 1).values),'classe'].values\n",
    "    print('Separando a base em treino e teste')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "    #Xsemi = MinMaxScaler().fit_transform(Xsemi)\n",
    "    print('Extraindo os melhores atributos dentre os %d atributos de X_train.' % X_train.shape[1])\n",
    "    estimator = DecisionTreeClassifier(random_state=1)\n",
    "    selector = RFECV(estimator, step=1, cv=5)\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    X_train = X_train[:,selector.support_]\n",
    "    X_test = X_test[:,selector.support_]\n",
    "    print('Número de atributos restantes após a operação: %d' % X_train.shape[1])\n",
    "\n",
    "    print('Extraindo os melhores atributos dentre os %d atributos de X.' % X.shape[1])\n",
    "    estimator = DecisionTreeClassifier(random_state=1)\n",
    "    selector = RFECV(estimator, step=1, cv=5)\n",
    "    selector = selector.fit(X, y)\n",
    "    X = X[:,selector.support_]\n",
    "    K = K[:,selector.support_]\n",
    "    print('Número de atributos restantes após a operação: %d' % X.shape[1])\n",
    "\n",
    "    #print('Adding semi supervising samples ......................................')\n",
    "    #X_train = np.concatenate((X_train, Xsemi[0:15000]))\n",
    "    #y_train = np.concatenate((y_train, np.full(15000, np.inf)))\n",
    "    #y_train = np.where(y_train==-1, 0, y_train)\n",
    "    #y_train = np.where(y_train==np.inf, -1, y_train)\n",
    "    #label_prop_model = LabelSpreading()\n",
    "    #y_train = label_prop_model.fit(X_train, y_train).predict(X_train)\n",
    "    #y_train = np.where(y_train==0, -1, y_train)\n",
    "\n",
    "    #print(X_train.shape)\n",
    "    #print(y_train.shape)\n",
    "    #np.savetxt(\"Xtrain.csv\", np.append(X_train, y_train[:,None], 1), delimiter=\",\")\n",
    "    \n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print('Imprimindo projeção em PCA.........................................')\n",
    "    pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "    projected = pca.fit_transform(X_train)\n",
    "    colors = ['r', 'b']\n",
    "    markers = ['x', 'x']\n",
    "    for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "        plt.scatter(X_train[y_train==l, 0], \n",
    "                    X_train[y_train==l, 1], \n",
    "                    c=c, label=l, marker=m)\n",
    "    plt.xlabel('PC 1')\n",
    "    plt.ylabel('PC 2')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "    projected = pca.fit_transform(X)\n",
    "    colors = ['r', 'b']\n",
    "    markers = ['x', 'x']\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        plt.scatter(X[y==l, 0], \n",
    "                    X[y==l, 1], \n",
    "                    c=c, label=l, marker=m)\n",
    "    plt.xlabel('PC 1')\n",
    "    plt.ylabel('PC 2')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    print('Imprimindo boxplot dos atributos.......................................')\n",
    "    df = pd.DataFrame(data=X_train)\n",
    "    df.boxplot(figsize=(15,7))\n",
    "    plt.show()\n",
    "\n",
    "    print(np.sum(y_train==1))\n",
    "    print(np.sum(y_train==0))\n",
    "    print(np.sum(y_train==-1))\n",
    "\n",
    "\n",
    "    print('\\nDimensao de X_train: ', X_train.shape)\n",
    "    print('\\nDimensao de X: ', X.shape)\n",
    "\n",
    "    print('\\nDimensao de y_train: ', y_train.shape)\n",
    "    print('\\nDimensao de y: ', y.shape)\n",
    "\n",
    "    print('\\nClasses do problema: ', np.unique(y_train))\n",
    "    print('\\nClasses do problema: ', np.unique(y))\n",
    "\n",
    "\n",
    "    duration = 0.5  # seconds\n",
    "    freq = 440  # Hz\n",
    "    os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    duration = 0.5  # seconds\n",
    "    freq = 440  # Hz\n",
    "    os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))\n",
    "    \n",
    "#except Exception as e:\n",
    "#    print(e)\n",
    "#    duration = 1.5  # seconds\n",
    "#    freq = 880  # Hz\n",
    "#    os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Experimento\n",
    "\n",
    "Nesta seção, o experimento deve ser conduzido, utilizando os protocolos experimentais ensinados no curso e executando os métodos inteligentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import  plot_confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "\n",
    "   \n",
    "cv = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "model = LogisticRegression(random_state=1, max_iter=15000)\n",
    "y_pred = model.fit(X_train, y_train).predict(X_train)\n",
    "print(\"LR Train:\",metrics.accuracy_score(y_train, y_pred))\n",
    "y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"LR Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "y_proba = model.fit(X_train, y_train).predict_proba(X_test)\n",
    "print(\"LR AUC:\",metrics.roc_auc_score(y_test, y_proba[:,1]))\n",
    "#print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "scores = cross_val_score(model, X_train, y_train, cv=cv)\n",
    "print('Cross Accuracy: %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "    \n",
    "model = RandomForestClassifier(max_depth=2, random_state=1)\n",
    "y_pred = model.fit(X_train, y_train).predict(X_train)\n",
    "print(\"RR Train:\",metrics.accuracy_score(y_train, y_pred))\n",
    "y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"RR Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "y_proba = model.fit(X_train, y_train).predict_proba(X_test)\n",
    "print(\"RR AUC:\",metrics.roc_auc_score(y_test, y_proba[:,1]))\n",
    "#print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Cross Accuracy: %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "print('Printing model to submission.csv')\n",
    "y_pred = model.fit(X, y).predict(K)\n",
    "y_pred_submission = model.fit(X, y).predict_proba(K)[:,1]\n",
    "result = np.zeros((K.shape[0],2))\n",
    "for i in range(K.shape[0]):\n",
    "    result[i][0] = test_dataset.iloc[:,:].values.T[0][i]\n",
    "    result[i][1] = y_pred_submission[i]\n",
    "resultdf = pd.DataFrame(data=result, columns=[\"Id\", \"Predicted\"])\n",
    "resultdf['Id'] = resultdf['Id'].astype(int)\n",
    "resultdf['Predicted'] = resultdf['Predicted'].round(decimals=5)\n",
    "resultdf.to_csv('submission.csv', index=False, float_format='%.5f')\n",
    "\n",
    "model = MultinomialNB()\n",
    "y_pred = model.fit(X_train, y_train).predict(X_train)\n",
    "print(\"MNB Train:\",metrics.accuracy_score(y_train, y_pred))\n",
    "y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"MNB Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "y_proba = model.fit(X_train, y_train).predict_proba(X_test)\n",
    "print(\"MNB AUC:\",metrics.roc_auc_score(y_test, y_proba[:,1]))\n",
    "#print(classification_report(y_test, y_pred))\n",
    "#plot_confusion_matrix(model, X_test, y_test)\n",
    "#plt.show()\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Cross Accuracy: %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "\n",
    "\n",
    "model = svm.SVC(decision_function_shape='ovo', probability=True,random_state=1)\n",
    "y_pred = model.fit(X_train, y_train).predict(X_train)\n",
    "print(\"SVM Train:\",metrics.accuracy_score(y_train, y_pred))\n",
    "y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "y_proba = model.fit(X_train, y_train).predict_proba(X_test)\n",
    "print(\"SVM AUC:\",metrics.roc_auc_score(y_test, y_proba[:,1]))\n",
    "#print(classification_report(y_test, y_pred))\n",
    "#plot_confusion_matrix(model, X_test, y_test)\n",
    "#plt.show()\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Cross Accuracy: %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "\n",
    "if True:    \n",
    "    model = KNeighborsClassifier()\n",
    "    y_pred = model.fit(X_train, y_train).predict(X_train)\n",
    "    print(\"KNN Train:\",metrics.accuracy_score(y_train, y_pred))\n",
    "    y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "    print(\"KNN Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "    y_proba = model.fit(X_train, y_train).predict_proba(X_test)\n",
    "    print(\"SVM AUC:\",metrics.roc_auc_score(y_test, y_proba[:,1]))\n",
    "    #print(classification_report(y_test, y_pred))\n",
    "    #plot_confusion_matrix(model, X_test, y_test)\n",
    "    #plt.show()\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    print('Cross Accuracy: %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "    model = MLPClassifier( alpha=1e-5, hidden_layer_sizes=(300,), random_state=1, max_iter=5000)\n",
    "    y_pred = model.fit(X_train, y_train).predict(X_train)\n",
    "    print(\"NN Train:\",metrics.accuracy_score(y_train, y_pred))\n",
    "    y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "    print(\"NN Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "    y_proba = model.fit(X_train, y_train).predict_proba(X_test)\n",
    "    print(\"SVM AUC:\",metrics.roc_auc_score(y_test, y_proba[:,1]))\n",
    "    #print(classification_report(y_test, y_pred))\n",
    "    #plot_confusion_matrix(model, X_test, y_test)\n",
    "    #plt.show()\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    print('Cross Accuracy: %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "    \n",
    "duration = 0.5  # seconds\n",
    "freq = 440  # Hz\n",
    "os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "time.sleep(0.5)\n",
    "    \n",
    "duration = 0.5  # seconds\n",
    "freq = 440  # Hz\n",
    "os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "time.sleep(0.5)\n",
    "    \n",
    "duration = 0.5  # seconds\n",
    "freq = 440  # Hz\n",
    "os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise dos Resultados\n",
    "\n",
    "Nesta seção, os resultados devem ser exibidos e comparados, através de tabelas e gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
