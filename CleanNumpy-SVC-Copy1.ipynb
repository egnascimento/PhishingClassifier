{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"figs/LogoUFSCar.jpg\" alt=\"Logo UFScar\" width=\"110\" align=\"left\"/>  <br/> <center>Universidade Federal de São Carlos (UFSCar)<br/><font size=\"4\"> Departamento de Computação, campus Sorocaba</center></font>\n",
    "</p>\n",
    "\n",
    "<font size=\"4\"><center><b>Disciplina: Aprendizado de Máquina</b></center></font>\n",
    "  \n",
    "<font size=\"3\"><center>Prof. Dr. Tiago A. Almeida</center></font>\n",
    "\n",
    "## <center>Projeto Final</center>\n",
    "\n",
    "**Aluno**: Eduardo Garcia do Nascimento\n",
    "\n",
    "**RA/CPF**: 22008732800\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise exploratória\n",
    "\n",
    "Nesta seção, deve ser feita a leitura da base de dados e todas as análises necessárias para entendê-la melhor, tais como:\n",
    "* Significado de cada atributo\n",
    "* Medidas descritivas\n",
    "* Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1     2     1]\n",
      " [    2     1     2]\n",
      " [40000     2     4]\n",
      " [    4     2     4]\n",
      " [    4     2     4]\n",
      " [    4     2     4]\n",
      " [    4     2     4]\n",
      " [40000     2     4]\n",
      " [    4     2     4]\n",
      " [    4     2     4]\n",
      " [    4     2     4]\n",
      " [    2     3     2]]\n",
      "[[0.4473679  0.         2.32933605]\n",
      " [0.44730081 2.44948974 1.36547286]\n",
      " [2.23606797 0.         0.56225353]\n",
      " [0.44716663 0.         0.56225353]\n",
      " [0.44716663 0.         0.56225353]\n",
      " [0.44716663 0.         0.56225353]\n",
      " [0.44716663 0.         0.56225353]\n",
      " [2.23606797 0.         0.56225353]\n",
      " [0.44716663 0.         0.56225353]\n",
      " [0.44716663 0.         0.56225353]\n",
      " [0.44716663 0.         0.56225353]\n",
      " [0.44730081 2.44948974 1.36547286]]\n",
      "Amostras mantidas: 12 de 12\n",
      "[[    1     2     1]\n",
      " [    2     1     2]\n",
      " [40000     2     4]\n",
      " [    4     2     4]\n",
      " [    4     2     4]\n",
      " [    4     2     4]\n",
      " [    4     2     4]\n",
      " [40000     2     4]\n",
      " [    4     2     4]\n",
      " [    4     2     4]\n",
      " [    4     2     4]\n",
      " [    2     3     2]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'asdffas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d2329d0b03a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0masdffas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# importa o arquivo e guarda em um dataframe do Pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asdffas' is not defined"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Caminho dos arquivos\n",
    "FILES_DIRECTORY = \"data\"\n",
    "\n",
    "import numpy as np  # importa a biblioteca usada para trabalhar com vetores e matrizes\n",
    "import pandas as pd # importa a biblioteca usada para trabalhar com dataframes (dados em formato de tabela) e análise de dados\n",
    "import os # importa a biblioteca para tarefas relacionadas ao sistema operacional\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    X = np.array([[1,2,1],[2,1,2],[40000,2,4],[4,2,4],[4,2,4],[4,2,4],[4,2,4],[40000,2,4],[4,2,4],[4,2,4],[4,2,4],[2,3,2]])\n",
    "    print(X)\n",
    "    z = np.abs(stats.zscore(X))\n",
    "    print(z)\n",
    "    z = np.nan_to_num(z)\n",
    "    print('Amostras mantidas: %d de %d' % (np.sum((z < 3).all(axis=1)), X.shape[0]))\n",
    "    X = X[(z < 3).all(axis=1)]\n",
    "    \n",
    "    print(X)\n",
    "    asdffas\n",
    "                       \n",
    "    # importa o arquivo e guarda em um dataframe do Pandas\n",
    "    set1_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set1.csv'), sep=',', low_memory=False)\n",
    "    set2_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set2.csv'), sep=',', low_memory=False) \n",
    "    set3_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set3.csv'), sep=',', low_memory=False) \n",
    "\n",
    "    # Concatena os datasets em somente um\n",
    "    frames = [ set1_dataset,set2_dataset,set3_dataset ]\n",
    "    input_dataset = pd.concat(frames, axis=1)\n",
    "\n",
    "    # Concatena as classes junto ao dataframe de atributos\n",
    "    train_dataset = pd.read_csv(os.path.join(FILES_DIRECTORY, 'train.csv'), sep=',')\n",
    "    input_dataset['classe'] = np.nan\n",
    "    input_dataset.loc[train_dataset['Id'].values,'classe'] = train_dataset['Class'].values\n",
    "\n",
    "    print('Dados concatenados produzindo um total de %d atributos' % \n",
    "            input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)].shape[1])\n",
    "    \n",
    "    print('O número de amostras com classificação válida é: %d' % \n",
    "            input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)].shape[0])\n",
    "\n",
    "    print('Dados de treinamento carregados com sucesso!')\n",
    "\n",
    "    test_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'test.csv'), sep=',')\n",
    "\n",
    "    print('Dados de teste carregados com sucesso!')\n",
    "   \n",
    "    duration = 0.5  # seconds\n",
    "    freq = 440  # Hz\n",
    "    os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Pré-processamento\n",
    "\n",
    "Nesta seção, as funções da etapa de pré-processamento dos dados devem ser implementadas e aplicadas (se necessário)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "import utils\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "if True:\n",
    "    print('Preenchendo arrays a partir dos datasets de entrada com dados cuja a classe é definida')\n",
    "    X = input_dataset.drop('classe', axis=1).drop('Id', axis=1).values\n",
    "    y = input_dataset[['classe']].values\n",
    "    print('X',X.shape)\n",
    "    print('y',y.shape)\n",
    "\n",
    "    print('Removendo atributos com baixa variância....................................')\n",
    "    variance_mask = VarianceThreshold().fit(X).get_support()\n",
    "    X = X[:,variance_mask]\n",
    "    print('Atributos removidos por baixa variância: %d' % np.sum(~variance_mask))\n",
    "\n",
    "    \n",
    "    #print('Removendo atributos com alta correlação....................................')\n",
    "    #X, corr_mask = utils.remove_correlated(X, 0.95)\n",
    "    #print('Atributos removido por alta correlação: %d' % len(corr_mask))\n",
    "\n",
    "    \n",
    "    X = utils.clean_dataset(X)\n",
    "    \n",
    "\n",
    "    print('Normalizing scale between 0 and 1..................................')\n",
    "    #X = MinMaxScaler().fit_transform(X)\n",
    "    K = X[test_dataset.iloc[:,:].values.T[0]]\n",
    "    \n",
    "    print('Removendo atributos com baixa variância....................................')\n",
    "    variance_mask = VarianceThreshold().fit(X).get_support()\n",
    "    X = X[:,variance_mask]\n",
    "    K = K[:,variance_mask]\n",
    "    print('Atributos removidos por baixa variância: %d' % np.sum(~variance_mask))\n",
    "\n",
    "    print('Removendo outliers..................................')\n",
    "    z = np.abs(stats.zscore(X))\n",
    "    z = np.nan_to_num(z)\n",
    "    print('Amostras mantidas: %d de %d' % (np.sum((z < 3).all(axis=1)), X.shape[0]))\n",
    "    X = X[(z < 3).all(axis=1)]\n",
    "    \n",
    "    \n",
    "    print('Balanceando amostras das classes: Antes: -1=%d 1=%d' % (np.sum(y==-1), np.sum(y==1)))\n",
    "    X,y = utils.balance_classes(X,y)\n",
    "    print('Balanceando amostras das classes: Depois: -1=%d 1=%d' % (np.sum(y==-1), np.sum(y==1)))\n",
    "    \n",
    "    \n",
    "    #Xsemi = input_dataset.loc[(input_dataset['classe'] != -1) & (input_dataset['classe'] != 1)].drop('classe', axis=1).drop('Id', axis=1).values\n",
    "    #mask = ((y == -1) | (y == 1))\n",
    "    #X = X[mask]\n",
    "    #y = y[mask]\n",
    "    X = utils.clean_dataset(X)\n",
    "    print(X.shape, y.shape)\n",
    "    print('Separando a base em treino e teste')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0, stratify=y)\n",
    "\n",
    "    #Xsemi = MinMaxScaler().fit_transform(Xsemi)\n",
    "    print('Extraindo os melhores atributos dentre os %d atributos de X_train.' % X_train.shape[1])\n",
    "    estimator = svm.SVC(kernel='linear', class_weight='balanced', probability=True,random_state=1)\n",
    "    selector = RFECV(estimator, step=1, cv=5)\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    print(selector.support_)\n",
    "    X_train = X_train[:,selector.support_]\n",
    "    X_test = X_test[:,selector.support_]\n",
    "    print('Número de atributos restantes após a operação: %d' % X_train.shape[1])\n",
    "\n",
    "    print('Extraindo os melhores atributos dentre os %d atributos de X.' % X.shape[1])\n",
    "    estimator = svm.SVC(kernel='linear', class_weight='balanced', probability=True,random_state=1)\n",
    "    selector = RFECV(estimator, step=1, cv=5)\n",
    "    selector = selector.fit(X, y)\n",
    "    print(selector.support_)\n",
    "    X = X[:,selector.support_]\n",
    "    K = K[:,selector.support_]\n",
    "    print('Número de atributos restantes após a operação: %d' % X.shape[1])\n",
    "\n",
    "    #print('Adding semi supervising samples ......................................')\n",
    "    #X_train = np.concatenate((X_train, Xsemi[0:15000]))\n",
    "    #y_train = np.concatenate((y_train, np.full(15000, np.inf)))\n",
    "    #y_train = np.where(y_train==-1, 0, y_train)\n",
    "    #y_train = np.where(y_train==np.inf, -1, y_train)\n",
    "    #label_prop_model = LabelSpreading()\n",
    "    #y_train = label_prop_model.fit(X_train, y_train).predict(X_train)\n",
    "    #y_train = np.where(y_train==0, -1, y_train)\n",
    "\n",
    "    #print(X_train.shape)\n",
    "    #print(y_train.shape)\n",
    "    #np.savetxt(\"Xtrain.csv\", np.append(X_train, y_train[:,None], 1), delimiter=\",\")\n",
    "    \n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print('Imprimindo projeção em PCA.........................................')\n",
    "    pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "    projected = pca.fit_transform(X_train)\n",
    "    colors = ['r', 'b']\n",
    "    markers = ['x', 'x']\n",
    "    for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "        plt.scatter(X_train[y_train==l, 0], \n",
    "                    X_train[y_train==l, 1], \n",
    "                    c=c, label=l, marker=m)\n",
    "    plt.xlabel('PC 1')\n",
    "    plt.ylabel('PC 2')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "    projected = pca.fit_transform(X)\n",
    "    colors = ['r', 'b']\n",
    "    markers = ['x', 'x']\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        plt.scatter(X[y==l, 0], \n",
    "                    X[y==l, 1], \n",
    "                    c=c, label=l, marker=m)\n",
    "    plt.xlabel('PC 1')\n",
    "    plt.ylabel('PC 2')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    print('Imprimindo boxplot dos atributos.......................................')\n",
    "    df = pd.DataFrame(data=X_train)\n",
    "    df.boxplot(figsize=(15,7))\n",
    "    plt.show()\n",
    "    \n",
    "    X_train = utils.clean_dataset(X_train)\n",
    "    \n",
    "    df = pd.DataFrame(data=X_train)\n",
    "    df.boxplot(figsize=(15,7))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    print(np.sum(y_train==1))\n",
    "    print(np.sum(y_train==0))\n",
    "    print(np.sum(y_train==-1))\n",
    "\n",
    "\n",
    "    print('\\nDimensao de X_train: ', X_train.shape)\n",
    "    print('\\nDimensao de X: ', X.shape)\n",
    "\n",
    "    print('\\nDimensao de y_train: ', y_train.shape)\n",
    "    print('\\nDimensao de y: ', y.shape)\n",
    "\n",
    "    print('\\nClasses do problema: ', np.unique(y_train))\n",
    "    print('\\nClasses do problema: ', np.unique(y))\n",
    "\n",
    "\n",
    "    duration = 0.5  # seconds\n",
    "    freq = 440  # Hz\n",
    "    os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    duration = 0.5  # seconds\n",
    "    freq = 440  # Hz\n",
    "    os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))\n",
    "    \n",
    "#except Exception as e:\n",
    "#    print(e)\n",
    "#    duration = 1.5  # seconds\n",
    "#    freq = 880  # Hz\n",
    "#    os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Experimento\n",
    "\n",
    "Nesta seção, o experimento deve ser conduzido, utilizando os protocolos experimentais ensinados no curso e executando os métodos inteligentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import  plot_confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "   \n",
    "cv = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "print('--------------------------------------------------------------------------------------------------')\n",
    "model = LogisticRegression(random_state=1, max_iter=15000, class_weight='balanced')\n",
    "y_pred = model.fit(X_train, y_train).predict(X_train)\n",
    "print(\"LR Train:\",metrics.accuracy_score(y_train, y_pred))\n",
    "y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"LR Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "y_proba = model.fit(X_train, y_train).predict_proba(X_test)\n",
    "print(\"LR AUC:\",metrics.roc_auc_score(y_test, y_proba[:,1]))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "scores = cross_val_score(model, X, y, cv=cv)\n",
    "print('Cross Accuracy: %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "scores = cross_val_score(model, X_train, y_train, cv=cv)\n",
    "print('Cross Accuracy: %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "print('--------------------------------------------------------------------------------------------------')\n",
    "model = RandomForestClassifier(max_depth=2, random_state=1, class_weight='balanced')\n",
    "y_pred = model.fit(X_train, y_train).predict(X_train)\n",
    "print(\"RR Train:\",metrics.accuracy_score(y_train, y_pred))\n",
    "y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"RR Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "y_proba = model.fit(X_train, y_train).predict_proba(X_test)\n",
    "print(\"RR AUC:\",metrics.roc_auc_score(y_test, y_proba[:,1]))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Cross Accuracy: %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "\n",
    "print('--------------------------------------------------------------------------------------------------')\n",
    "model = MultinomialNB()\n",
    "y_pred = model.fit(X_train, y_train).predict(X_train)\n",
    "print(\"MNB Train:\",metrics.accuracy_score(y_train, y_pred))\n",
    "y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"MNB Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "y_proba = model.fit(X_train, y_train).predict_proba(X_test)\n",
    "print(\"MNB AUC:\",metrics.roc_auc_score(y_test, y_proba[:,1]))\n",
    "#print(classification_report(y_test, y_pred))\n",
    "#plot_confusion_matrix(model, X_test, y_test)\n",
    "#plt.show()\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Cross Accuracy: %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "\n",
    "print('--------------------------------------------------------------------------------------------------')\n",
    "model = svm.SVC(kernel='linear', class_weight='balanced', probability=True,random_state=1)\n",
    "y_pred = model.fit(X_train, y_train).predict(X_train)\n",
    "print(\"SVM Train:\",metrics.accuracy_score(y_train, y_pred))\n",
    "y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "y_proba = model.fit(X_train, y_train).predict_proba(X_test)\n",
    "print(\"SVM AUC:\",metrics.roc_auc_score(y_test, y_proba[:,1]))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Cross Accuracy: %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "print('Printing model to submission.csv.............................................')\n",
    "y_pred = model.fit(X, y).predict(K)\n",
    "y_pred_submission = model.fit(X, y).predict_proba(K)[:,1]\n",
    "result = np.zeros((K.shape[0],2))\n",
    "for i in range(K.shape[0]):\n",
    "    result[i][0] = test_dataset.iloc[:,:].values.T[0][i]\n",
    "    result[i][1] = y_pred_submission[i]\n",
    "resultdf = pd.DataFrame(data=result, columns=[\"Id\", \"Predicted\"])\n",
    "resultdf['Id'] = resultdf['Id'].astype(int)\n",
    "resultdf['Predicted'] = resultdf['Predicted'].round(decimals=5)\n",
    "resultdf.to_csv('submission.csv', index=False, float_format='%.5f')\n",
    "print('.............................................................................')\n",
    "\n",
    "\n",
    "print('--------------------------------------------------------------------------------------------------')\n",
    "if True:    \n",
    "    model = KNeighborsClassifier()\n",
    "    y_pred = model.fit(X_train, y_train).predict(X_train)\n",
    "    print(\"KNN Train:\",metrics.accuracy_score(y_train, y_pred))\n",
    "    y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "    print(\"KNN Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "    y_proba = model.fit(X_train, y_train).predict_proba(X_test)\n",
    "    print(\"SVM AUC:\",metrics.roc_auc_score(y_test, y_proba[:,1]))\n",
    "    #print(classification_report(y_test, y_pred))\n",
    "    #plot_confusion_matrix(model, X_test, y_test)\n",
    "    #plt.show()\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    print('Cross Accuracy: %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "    print('--------------------------------------------------------------------------------------------------')\n",
    "    model = MLPClassifier( alpha=1e-5, hidden_layer_sizes=(300,), random_state=1, max_iter=5000, )\n",
    "    y_pred = model.fit(X_train, y_train).predict(X_train)\n",
    "    print(\"NN Train:\",metrics.accuracy_score(y_train, y_pred))\n",
    "    y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "    print(\"NN Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "    y_proba = model.fit(X_train, y_train).predict_proba(X_test)\n",
    "    print(\"SVM AUC:\",metrics.roc_auc_score(y_test, y_proba[:,1]))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    #plot_confusion_matrix(model, X_test, y_test)\n",
    "    #plt.show()\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    print('Cross Accuracy: %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "duration = 0.5  # seconds\n",
    "freq = 440  # Hz\n",
    "os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "time.sleep(0.5)\n",
    "    \n",
    "duration = 0.5  # seconds\n",
    "freq = 440  # Hz\n",
    "os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "time.sleep(0.5)\n",
    "    \n",
    "duration = 0.5  # seconds\n",
    "freq = 440  # Hz\n",
    "os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise dos Resultados\n",
    "\n",
    "Nesta seção, os resultados devem ser exibidos e comparados, através de tabelas e gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
