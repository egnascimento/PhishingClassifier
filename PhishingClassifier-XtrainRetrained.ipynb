{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"figs/LogoUFSCar.jpg\" alt=\"Logo UFScar\" width=\"110\" align=\"left\"/>  <br/> <center>Universidade Federal de São Carlos (UFSCar)<br/><font size=\"4\"> Departamento de Computação, campus Sorocaba</center></font>\n",
    "</p>\n",
    "\n",
    "<font size=\"4\"><center><b>Disciplina: Aprendizado de Máquina</b></center></font>\n",
    "  \n",
    "<font size=\"3\"><center>Prof. Dr. Tiago A. Almeida</center></font>\n",
    "\n",
    "## <center>Projeto Final</center>\n",
    "\n",
    "**Aluno**: Eduardo Garcia do Nascimento\n",
    "\n",
    "**RA/CPF**: 22008732800\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise exploratória\n",
    "\n",
    "Nesta seção, deve ser feita a leitura da base de dados e todas as análises necessárias para entendê-la melhor, tais como:\n",
    "* Significado de cada atributo\n",
    "* Medidas descritivas\n",
    "* Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Caminho dos arquivos\n",
    "FILES_DIRECTORY = \"data\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from scripts import utils\n",
    "\n",
    "if __name__ == '__main__':\n",
    "                       \n",
    "    # importa o arquivo e guarda em um dataframe do Pandas\n",
    "    set1_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set1.csv'), sep=',', low_memory=False)\n",
    "    set2_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set2.csv'), sep=',', low_memory=False) \n",
    "    set3_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set3.csv'), sep=',', low_memory=False)\n",
    "    \n",
    "    # Renomeia colunas concatenando o setX antes de fazer o merge para identificá-las posteriormente\n",
    "    cols = set1_dataset.columns\n",
    "    for col in cols:\n",
    "        set1_dataset = set1_dataset.rename(columns={col:'set1_'+col})\n",
    "        \n",
    "    cols = set2_dataset.columns\n",
    "    for col in cols:\n",
    "        set2_dataset = set2_dataset.rename(columns={col:'set2_'+col})\n",
    "    \n",
    "    cols = set3_dataset.columns\n",
    "    for col in cols:\n",
    "        set3_dataset = set3_dataset.rename(columns={col:'set3_'+col})\n",
    "\n",
    "    # Concatena os datasets em somente um dataset único\n",
    "    frames = [ set1_dataset, set2_dataset, set3_dataset ]\n",
    "    input_dataset = pd.concat(frames, axis=1)\n",
    "    \n",
    "    print('A base de dados inicial combinada tem %d amostras com %d atributos' % (input_dataset.shape[0],\n",
    "                                                                                 input_dataset.shape[1]))\n",
    "    \n",
    "    # Remove os atributos que são constantes e não oferecem nenhum valor aos algoritmos de classificação\n",
    "    variance_mask = VarianceThreshold().fit(input_dataset).get_support()\n",
    "    input_dataset = input_dataset.iloc[:,variance_mask]\n",
    "    print('Atributos removidos por baixa variância: %d' % np.sum(~variance_mask))\n",
    "    \n",
    "    # Tratamento de outliers e entradas nulas\n",
    "    df = input_dataset.copy()\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))\n",
    "    print(\"Número de outliers substituídos por valores nulos:\", np.sum(np.sum(mask)))\n",
    "    #df[mask] = np.nan\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    input_dataset.loc[:,:] = imp_mean.fit_transform(input_dataset)\n",
    "    \n",
    "    # Normalização dos dados entre 0 e 1\n",
    "    input_dataset.loc[:,:] = MinMaxScaler().fit_transform(input_dataset)\n",
    "    \n",
    "    # Adiciona as classes junto ao dataset de atributos\n",
    "    train_dataset = pd.read_csv(os.path.join(FILES_DIRECTORY, 'train.csv'), sep=',')\n",
    "    input_dataset['classe'] = np.nan\n",
    "    input_dataset.loc[train_dataset['Id'].values,'classe'] = train_dataset['Class'].values\n",
    "    \n",
    "    backup_dataset = input_dataset.copy()\n",
    "    \n",
    "    mask = ((input_dataset['classe'] == -1) | (input_dataset['classe'] == 1))\n",
    "    display(input_dataset.loc[mask].head(10))\n",
    "    \n",
    "    cols = list(input_dataset.columns)\n",
    "    cols.remove('classe')\n",
    "        \n",
    "    # Seleciona os melhores atritubos para treinametno do algoritmo de classificação\n",
    "    print('Selecionando melhores features....................................')\n",
    "    selector = SelectKBest(f_classif, k=6).fit(\n",
    "        input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1), cols].values,\n",
    "        input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1),'classe'].values)\n",
    "    mask = selector.get_support()\n",
    "    mask = np.append(mask, True)\n",
    "    input_dataset = input_dataset.iloc[:,mask]\n",
    "    \n",
    "    # matriz de gráficos scatter \n",
    "    sns.pairplot(input_dataset, hue='classe', height=3.5);\n",
    "    plt.show()\n",
    "    \n",
    "    # matrizes de covariancia e correlação\n",
    "    df_covariance = input_dataset.iloc[:,:-1].cov()\n",
    "    df_correlation = input_dataset.iloc[:,:-1].corr()\n",
    "    \n",
    "    \n",
    "    # cria um mapa de cores dos valoes da covariancia\n",
    "    sns.heatmap(df_covariance, annot=True, xticklabels=df_correlation.columns, yticklabels=df_correlation.columns)\n",
    "    plt.title('Covariância')\n",
    "    plt.show()\n",
    "\n",
    "    # cria um mapa de cores dos valoes da correlação\n",
    "    sns.heatmap(df_correlation, annot=True, xticklabels=df_correlation.columns, yticklabels=df_correlation.columns)\n",
    "    plt.title('Correlação')\n",
    "    plt.show()\n",
    "    \n",
    "    display(input_dataset.info())\n",
    "    \n",
    "    print('Dados concatenados produzindo um total de %d atributos' % \n",
    "            input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)].shape[1])\n",
    "    \n",
    "    print('O número de amostras com classificação válida é: %d' % \n",
    "            input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)].shape[0])\n",
    "\n",
    "    print('Dados de treinamento carregados com sucesso!')\n",
    "\n",
    "    test_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'test.csv'), sep=',')\n",
    "    K = input_dataset.loc[test_dataset.iloc[:,:].values.T[0]]\n",
    "    K = K.drop('classe', axis=1).values\n",
    "    \n",
    "    print('Análise e visualização dos dados:')\n",
    "    y = input_dataset.classe\n",
    "    ax = sns.countplot(y, label=\"Contagem\")\n",
    "    N,U,P = y.value_counts()\n",
    "    print('Número de posts comuns:', N)\n",
    "    print('Número de posts não reconhecidos:', U)\n",
    "    print('Número de posts phishing', P)\n",
    "    plt.show()\n",
    "    \n",
    "    display(input_dataset.describe())\n",
    "    \n",
    "    cl = input_dataset.columns\n",
    "    cl = cl.drop('classe')\n",
    "    \n",
    " \n",
    "    plt.figure(figsize=(20,10))\n",
    "    data = pd.melt(input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)],\n",
    "                   id_vars=\"classe\", var_name=\"features\", value_name='value')\n",
    "    sns.boxplot(x='features', y='value', hue='classe', data=data)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    sns.violinplot(x='features', y='value', hue='classe', data=data, split=True, inner=\"quartile\")\n",
    "    plt.show()\n",
    "    \n",
    "    X = input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)]\n",
    "    X_total = X.drop('classe', axis=1).values\n",
    "    y_total = input_dataset.loc[((input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)), 'classe'].values\n",
    "    \n",
    "    utils.printPCA(X_total,y_total)    \n",
    "    \n",
    "    utils.beep(1, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Pré-processamento\n",
    "\n",
    "Nesta seção, as funções da etapa de pré-processamento dos dados devem ser implementadas e aplicadas (se necessário)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scripts import utils\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "evaluate_svm_hiperparameters = False # Busca os melhores parâmetros para as máquinas de vetores de suporte\n",
    "evaluate_rfc_hiperparameters = False # Busca os melhores parâmetros para as florestas aleatórias\n",
    "evaluate_lrc_hiperparameters = False # Busca os melhores parâmetros para a regressão linear\n",
    "\n",
    "print('Removendo amostras outliers')\n",
    "X, y = utils.remove_outliers(X_total, y_total)\n",
    "utils.printPCA(X,y)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print('Separando a base em treino e teste')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "\n",
    "utils.printBoxPlot(X)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=4, random_state=1, shuffle=True)\n",
    "\n",
    "scores = ['balanced_accuracy', 'f1', 'roc_auc']\n",
    "\n",
    "if evaluate_svm_hiperparameters == True:\n",
    "    print('Buscando os melhores parâmetros para as máquinas de vetores de suporte:')\n",
    "    tuned_parameters = [{'kernel': ['rbf'], 'gamma': ['scale', 1e-3, 1e-4], 'C': [1, 10, 100, 200, 400, 600, 1000]},\n",
    "                        {'kernel': ['linear'], 'C': [1, 10, 100, 200, 400, 600, 1000]},\n",
    "                        {'kernel': ['poly'], 'C': [1, 10, 100, 200, 400, 600, 1000], 'degree': [2,3,4,5,6], 'gamma': ['scale', 1e-3, 1e-4]}]\n",
    "\n",
    "    \n",
    "\n",
    "    for score in scores:\n",
    "        clf = GridSearchCV(\n",
    "                svm.SVC(class_weight='balanced', random_state=1), tuned_parameters, scoring=score, cv=cv\n",
    "            )\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        print('Os melhores parâmetros encontrados para a pontuação %s foram:' % score)\n",
    "        print(clf.best_params_)\n",
    "\n",
    "    \n",
    "if evaluate_rfc_hiperparameters == True:    \n",
    "    print('Buscando os melhores parâmetros para as florestas aleatórias:')\n",
    "\n",
    "    tuned_parameters = { 'n_estimators': [100, 200, 500],\n",
    "                         'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                         'max_depth' : [None, 2,3,4,5,6,7,8],\n",
    "                         'criterion' :['gini', 'entropy']}\n",
    "\n",
    "    for score in scores:\n",
    "        clf = GridSearchCV(\n",
    "                RandomForestClassifier(class_weight='balanced'), tuned_parameters, scoring=score, cv=cv\n",
    "            )\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        print('Os melhores parâmetros encontrados para a pontuação %s foi:' % score)\n",
    "        print(clf.best_params_)\n",
    "\n",
    "utils.beep(2, 600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Experimento\n",
    "\n",
    "Nesta seção, o experimento deve ser conduzido, utilizando os protocolos experimentais ensinados no curso e executando os métodos inteligentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import  plot_confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "\n",
    "submit_by_preference = True\n",
    "submit_preference_score = 'SVM rbf_ra'\n",
    "\n",
    "submit_by_score = False\n",
    "submit_best_score = 'balanced_accuracy'\n",
    "\n",
    "scoring_list=['roc_auc', 'f1', 'f1_micro', 'f1_macro', 'f1_weighted', 'accuracy', 'balanced_accuracy']\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "model_list = [\n",
    "    ['SVM poly', svm.SVC(kernel='poly', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "    ['SVM linear', svm.SVC(kernel='linear', C=100, class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "    ['SVM rbf', svm.SVC(kernel='rbf', C=1000, gamma=0.0001, class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "    #['SVM poly_ra', svm.SVC(kernel='poly', C=1000, degree=6, gamma='scale', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "    ['SVM rbf_ra', svm.SVC(kernel='rbf', C=10, gamma='scale', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "    ['SVM rbf_ba', svm.SVC(kernel='rbf', C=400, gamma='scale', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "    ['SVM rbf_f1', svm.SVC(kernel='rbf', C=600, gamma=0.001, class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "    ['Random Forest', RandomForestClassifier(max_depth=2, class_weight='balanced', random_state=1), 0],\n",
    "    ['Logistic Regression', LogisticRegression(random_state=1, class_weight='balanced', max_iter=15000), 0],\n",
    "    ['Multinomial NB', MultinomialNB(), 0],\n",
    "    ['KNN', KNeighborsClassifier(weights='distance'), 0]\n",
    "]\n",
    "\n",
    "# Pré balanceamento dos dados utilizando a técnica de oversampling\n",
    "X_train_bal, y_train_bal = utils.balance_classes(X_train, y_train)\n",
    "X_bal = X\n",
    "y_bal = y\n",
    "#X_bal, y_bal = utils.balance_classes(X, y)\n",
    "\n",
    "semi_model = svm.SVC(kernel='poly', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1)\n",
    "if True:\n",
    "    # População dos dados não classificados com o melhor classificador encontrado com os resultados mais confiáveis\n",
    "    X_semi_df = backup_dataset[(backup_dataset.classe!=-1)&(backup_dataset.classe!=1)&(backup_dataset.classe!=0)]\n",
    "    X_semi_df = X_semi_df.iloc[:,mask]\n",
    "    model = semi_model\n",
    "    clf = model.fit(X_train_bal, y_train_bal)\n",
    "    y_semi = clf.predict(X_semi_df.drop('classe', axis=1).values)\n",
    "    y_probas = clf.predict_proba(X_semi_df.drop('classe', axis=1).values)\n",
    "    proba_mask = (y_probas[:,0] < 0.2) | (y_probas[:,0] > 0.8)\n",
    "    y_semi = y_semi[proba_mask]\n",
    "    X_semi_df = X_semi_df[proba_mask]\n",
    "    X_semi = X_semi_df.drop('classe', axis=1).values\n",
    "\n",
    "    print('Número de amostras acrescentadas ao dataset de treinamento:', X_semi.shape[0], \n",
    "                                                                          np.sum(y_semi==-1), np.sum(y_semi==1))\n",
    "    X_train_bal = np.concatenate([X_train_bal, X_semi], axis=0)\n",
    "    y_train_bal = np.concatenate([y_train_bal, y_semi], axis=0)\n",
    "    X_bal = np.concatenate([X_bal, X_semi], axis=0)\n",
    "    y_bal = np.concatenate([y_bal, y_semi], axis=0)\n",
    "\n",
    "if True:\n",
    "    X_train_bal, y_train_bal = utils.balance_classes(X_train_bal, y_train_bal)\n",
    "    X_bal, y_bal = utils.balance_classes(X_bal, y_bal)\n",
    "\n",
    "    # População dos dados não classificados com o melhor classificador encontrado com os resultados mais confiáveis\n",
    "    X_semi_df = backup_dataset[(backup_dataset.classe!=-1)&(backup_dataset.classe!=1)&(backup_dataset.classe!=0)]\n",
    "    X_semi_df = X_semi_df[~proba_mask]\n",
    "    X_semi_df = X_semi_df.iloc[:,mask]\n",
    "    model = semi_model\n",
    "    clf = model.fit(X_train_bal, y_train_bal)\n",
    "    y_semi = clf.predict(X_semi_df.drop('classe', axis=1).values)\n",
    "    y_probas = clf.predict_proba(X_semi_df.drop('classe', axis=1).values)\n",
    "    proba_mask = (y_probas[:,0] < 0.2) | (y_probas[:,0] > 0.8)\n",
    "    y_semi = y_semi[proba_mask]\n",
    "    X_semi_df = X_semi_df[proba_mask]\n",
    "    X_semi = X_semi_df.drop('classe', axis=1).values\n",
    "    print('Número de amostras acrescentadas ao dataset de treinamento:', X_semi.shape[0], \n",
    "                                                                          np.sum(y_semi==-1), np.sum(y_semi==1))\n",
    "    X_train_bal = np.concatenate([X_train_bal, X_semi], axis=0)\n",
    "    y_train_bal = np.concatenate([y_train_bal, y_semi], axis=0)\n",
    "    X_bal = np.concatenate([X_bal, X_semi], axis=0)\n",
    "    y_bal = np.concatenate([y_bal, y_semi], axis=0)\n",
    "\n",
    "#X_train_bal, y_train_bal = utils.balance_classes(X_train_bal, y_train_bal)\n",
    "#X_bal, y_bal = utils.balance_classes(X_bal, y_bal)\n",
    "\n",
    "# Configura K folds estratificados, ou seja, mantendo as mesmas proporções entre classes\n",
    "cv = StratifiedKFold(n_splits=4, random_state=1, shuffle=True)\n",
    "\n",
    "\n",
    "for model in model_list:\n",
    "    print(model[0] + '------------------------------------------------------------------------------------')\n",
    "    y_pred = utils.evaluate_model(model[1], X_train_bal, y_train_bal, X_test, y_test)\n",
    "    \n",
    "    for scoring in scoring_list:\n",
    "        scores = cross_val_score(model[1], X_bal, y_bal, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "        print('Cross ' + scoring + ': %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "        results.loc[model[0], scoring] = np.mean(scores)\n",
    "        if scoring == submit_best_score:\n",
    "            model[2] = np.mean(scores)\n",
    "    results.loc[model[0], 'TN'] = confusion_matrix(y_test, y_pred)[0][0]\n",
    "    results.loc[model[0], 'TP'] = confusion_matrix(y_test, y_pred)[1][1]\n",
    "\n",
    "\n",
    "if submit_by_score == True:\n",
    "    best_score = 0\n",
    "    best_model = 0\n",
    "    best_model_name = 'None'\n",
    "    for model in model_list:\n",
    "        if model[2]>best_score:\n",
    "            best_score = model[2]\n",
    "            best_model = model[1]\n",
    "            best_model_name = model[0]\n",
    "\n",
    "    print('O modelo que obteve melhores resultados foi: ' + \n",
    "      best_model_name + ' com ' + \n",
    "      submit_best_score + '=' + str(best_score))\n",
    "    \n",
    "if submit_by_preference == True:\n",
    "    for model in model_list:\n",
    "        if model[0]==submit_preference_score:\n",
    "            best_score = model[2]\n",
    "            best_model = model[1]\n",
    "            best_model_name = model[0]\n",
    "\n",
    "    print('O modelo que obteve melhores resultados foi: ' + \n",
    "      best_model_name + ' com ' + \n",
    "      submit_preference_score + '=' + str(best_score))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print('Printing model to submission.csv ###################################################################')\n",
    "clf= best_model.fit(X_bal, y_bal)\n",
    "y_pred_submission = clf.predict_proba(K)[:,1]\n",
    "result = np.zeros((K.shape[0],2))\n",
    "for i in range(K.shape[0]):\n",
    "    result[i][0] = test_dataset.iloc[:,:].values.T[0][i]\n",
    "    result[i][1] = y_pred_submission[i]\n",
    "resultdf = pd.DataFrame(data=result, columns=[\"Id\", \"Predicted\"])\n",
    "resultdf['Id'] = resultdf['Id'].astype(int)\n",
    "resultdf['Predicted'] = resultdf['Predicted'].round(decimals=5)\n",
    "resultdf.to_csv('submission.csv', index=False, float_format='%.5f')\n",
    "print('####################################################################################################')\n",
    "\n",
    "display(results)\n",
    "    \n",
    "utils.beep(3, 800)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise dos Resultados\n",
    "\n",
    "Nesta seção, os resultados devem ser exibidos e comparados, através de tabelas e gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asdfqwerqwrqwerqwer\n",
    "qeqwerqwer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
