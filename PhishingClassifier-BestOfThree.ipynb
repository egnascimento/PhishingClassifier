{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"figs/LogoUFSCar.jpg\" alt=\"Logo UFScar\" width=\"110\" align=\"left\"/>  <br/> <center>Universidade Federal de São Carlos (UFSCar)<br/><font size=\"4\"> Departamento de Computação, campus Sorocaba</center></font>\n",
    "</p>\n",
    "\n",
    "<font size=\"4\"><center><b>Disciplina: Aprendizado de Máquina</b></center></font>\n",
    "  \n",
    "<font size=\"3\"><center>Prof. Dr. Tiago A. Almeida</center></font>\n",
    "\n",
    "## <center>Projeto Final</center>\n",
    "\n",
    "**Aluno**: Eduardo Garcia do Nascimento\n",
    "\n",
    "**RA/CPF**: 22008732800\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Carregamento dos dados\n",
    "\n",
    "Nesta seção é feita a carga dos atributos em um dataframe só, ou seja, os três datasets são lidos e concatenados para que a redução de atributos leve em conta o que existe de melhor em todos eles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Caminho dos arquivos\n",
    "FILES_DIRECTORY = \"data\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from scripts import utils\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "                      \n",
    "    # importa o arquivo e guarda em um dataframe do Pandas\n",
    "    set1_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set1.csv'), sep=',', low_memory=False)\n",
    "    set2_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set2.csv'), sep=',', low_memory=False) \n",
    "    set3_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set3.csv'), sep=',', low_memory=False)\n",
    "    train_dataset = pd.read_csv(os.path.join(FILES_DIRECTORY, 'train.csv'), sep=',')\n",
    "    test_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'test.csv'), sep=',')\n",
    "    \n",
    "    # Renomeia colunas concatenando o setX antes de fazer o merge para identificá-las posteriormente\n",
    "    cols = set1_dataset.columns\n",
    "    for col in cols:\n",
    "        set1_dataset = set1_dataset.rename(columns={col:'set1_'+col})\n",
    "        \n",
    "    cols = set2_dataset.columns\n",
    "    for col in cols:\n",
    "        set2_dataset = set2_dataset.rename(columns={col:'set2_'+col})\n",
    "    \n",
    "    cols = set3_dataset.columns\n",
    "    for col in cols:\n",
    "        set3_dataset = set3_dataset.rename(columns={col:'set3_'+col})\n",
    "\n",
    "    # Concatena os datasets em somente um dataset único\n",
    "    frames = [ set3_dataset ]\n",
    "    combined_dataset = pd.concat(frames, axis=1)\n",
    "    \n",
    "    \n",
    "    set1_dataset['classe'] = np.nan\n",
    "    set2_dataset['classe'] = np.nan\n",
    "    set3_dataset['classe'] = np.nan\n",
    "    \n",
    "    \n",
    "    set1_dataset.loc[train_dataset['Id'].values,'classe'] = train_dataset['Class'].values\n",
    "    set2_dataset.loc[train_dataset['Id'].values,'classe'] = train_dataset['Class'].values\n",
    "    set3_dataset.loc[train_dataset['Id'].values,'classe'] = train_dataset['Class'].values\n",
    "    \n",
    "    #samples_mask = (input_dataset.classe!=-1)&(input_dataset.classe!=1)&(input_dataset.classe!=0)\n",
    "    #samples_df = input_dataset[samples_mask].drop('classe', axis=1)\n",
    "    \n",
    "    K1_df = set1_dataset.loc[test_dataset.iloc[:,:].values.T[0]].copy()\n",
    "    K1_df = K1_df.drop('classe', axis=1)\n",
    "    K2_df = set2_dataset.loc[test_dataset.iloc[:,:].values.T[0]].copy()\n",
    "    K2_df = K2_df.drop('classe', axis=1)\n",
    "    K3_df = set3_dataset.loc[test_dataset.iloc[:,:].values.T[0]].copy()\n",
    "    K3_df = K3_df.drop('classe', axis=1)\n",
    "    \n",
    "    #print('A base de dados inicial combinada tem %d amostras com %d atributos.' % (input_dataset.shape[0],\n",
    "    #                                                                             input_dataset.shape[1]))\n",
    "    \n",
    "        \n",
    "   \n",
    "    utils.beep(1, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Pré-processamento e seleção de atributos\n",
    "\n",
    "Nesta seção são feitas limpezas da base de dados como:\n",
    "\n",
    "* Remoção de atributos sem variância;\n",
    "* Tratamento de outliers e dados nulos;\n",
    "* Seleção dos atributos que terão maior valor para o algoritmo de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separando a base em treino e teste\n",
      "Atributos removidos por baixa variância: 3\n",
      "Atributos removidos por baixa variância: 2\n",
      "Atributos removidos por baixa variância: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "X1_df = set1_dataset.loc[(set1_dataset['classe'] == -1) | (set1_dataset['classe'] == 1)]\n",
    "X2_df = set2_dataset.loc[(set2_dataset['classe'] == -1) | (set2_dataset['classe'] == 1)]\n",
    "X3_df = set3_dataset.loc[(set3_dataset['classe'] == -1) | (set3_dataset['classe'] == 1)]\n",
    "\n",
    "print('Separando a base em treino e teste')\n",
    "X1_train_df, X1_test_df, y1_train_df, y1_test_df = train_test_split(X1_df.drop('classe', axis=1), \n",
    "                                                    X1_df[['classe']], \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=X1_df[['classe']],\n",
    "                                                    random_state=0, \n",
    "                                                    )\n",
    "X2_train_df, X2_test_df, y2_train_df, y2_test_df = train_test_split(X2_df.drop('classe', axis=1), \n",
    "                                                    X2_df[['classe']], \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=X2_df[['classe']],\n",
    "                                                    random_state=0, \n",
    "                                                    )\n",
    "X3_train_df, X3_test_df, y3_train_df, y3_test_df = train_test_split(X3_df.drop('classe', axis=1), \n",
    "                                                    X3_df[['classe']], \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=X3_df[['classe']],\n",
    "                                                    random_state=0, \n",
    "                                                    )\n",
    "\n",
    "y_df = X1_df[['classe']].copy()\n",
    "X1_df = X1_df.drop('classe', axis=1)\n",
    "X2_df = X2_df.drop('classe', axis=1)\n",
    "X3_df = X3_df.drop('classe', axis=1)\n",
    "\n",
    "# Remove os atributos que são constantes e não oferecem nenhum valor aos algoritmos de classificação\n",
    "variance_mask = VarianceThreshold().fit(X1_train_df).get_support()\n",
    "X1_train_df = X1_train_df.iloc[:,variance_mask]\n",
    "X1_test_df = X1_test_df.iloc[:,variance_mask]\n",
    "print('Atributos removidos por baixa variância: %d' % np.sum(~variance_mask))\n",
    "\n",
    "variance_mask = VarianceThreshold().fit(X2_train_df).get_support()\n",
    "X2_train_df = X2_train_df.iloc[:,variance_mask]\n",
    "X2_test_df = X2_test_df.iloc[:,variance_mask]\n",
    "print('Atributos removidos por baixa variância: %d' % np.sum(~variance_mask))\n",
    "\n",
    "variance_mask = VarianceThreshold().fit(X3_train_df).get_support()\n",
    "X3_train_df = X3_train_df.iloc[:,variance_mask]\n",
    "X3_test_df = X3_test_df.iloc[:,variance_mask]\n",
    "print('Atributos removidos por baixa variância: %d' % np.sum(~variance_mask))\n",
    "\n",
    "variance_mask = VarianceThreshold().fit(X1_df).get_support()\n",
    "X1_df = X1_df.iloc[:,variance_mask]\n",
    "K1_df = K1_df.iloc[:,variance_mask]\n",
    "variance_mask = VarianceThreshold().fit(X2_df).get_support()\n",
    "X2_df = X2_df.iloc[:,variance_mask]\n",
    "K2_df = K2_df.iloc[:,variance_mask]\n",
    "variance_mask = VarianceThreshold().fit(X3_df).get_support()\n",
    "X3_df = X3_df.iloc[:,variance_mask]\n",
    "K3_df = K3_df.iloc[:,variance_mask]\n",
    "\n",
    "# Tratamento de outliers e entradas nulas\n",
    "outliers_df = X1_train_df.copy()\n",
    "imp_mean = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp_mean = imp_mean.fit(outliers_df)\n",
    "X1_train_df.loc[:,:] = imp_mean.transform(X1_train_df)\n",
    "X1_test_df.loc[:,:] = imp_mean.transform(X1_test_df)\n",
    "outliers_df = X2_train_df.copy()\n",
    "imp_mean = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp_mean = imp_mean.fit(outliers_df)\n",
    "X2_train_df.loc[:,:] = imp_mean.transform(X2_train_df)\n",
    "X2_test_df.loc[:,:] = imp_mean.transform(X2_test_df)\n",
    "outliers_df = X3_train_df.copy()\n",
    "imp_mean = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp_mean = imp_mean.fit(outliers_df)\n",
    "X3_train_df.loc[:,:] = imp_mean.transform(X3_train_df)\n",
    "X3_test_df.loc[:,:] = imp_mean.transform(X3_test_df)\n",
    "\n",
    "\n",
    "outliers_df = X1_df.copy()\n",
    "imp_mean = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp_mean = imp_mean.fit(outliers_df)\n",
    "X1_df.loc[:,:] = imp_mean.transform(X1_df)\n",
    "K1_df.loc[:,:] = imp_mean.transform(K1_df)\n",
    "outliers_df = X2_df.copy()\n",
    "imp_mean = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp_mean = imp_mean.fit(outliers_df)\n",
    "X2_df.loc[:,:] = imp_mean.transform(X2_df)\n",
    "K2_df.loc[:,:] = imp_mean.transform(K2_df)\n",
    "outliers_df = X3_df.copy()\n",
    "imp_mean = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp_mean = imp_mean.fit(outliers_df)\n",
    "X3_df.loc[:,:] = imp_mean.transform(X3_df)\n",
    "K3_df.loc[:,:] = imp_mean.transform(K3_df)\n",
    "\n",
    "\n",
    "\n",
    "# Normalização dos dados entre 0 e 1\n",
    "scaler = MinMaxScaler().fit(X1_train_df)\n",
    "X1_train_df.loc[:,:] = scaler.transform(X1_train_df)\n",
    "X1_test_df.loc[:,:] = scaler.transform(X1_test_df)\n",
    "scaler = MinMaxScaler().fit(X2_train_df)\n",
    "X2_train_df.loc[:,:] = scaler.transform(X2_train_df)\n",
    "X2_test_df.loc[:,:] = scaler.transform(X2_test_df)\n",
    "scaler = MinMaxScaler().fit(X3_train_df)\n",
    "X3_train_df.loc[:,:] = scaler.transform(X3_train_df)\n",
    "X3_test_df.loc[:,:] = scaler.transform(X3_test_df)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler().fit(X1_df)\n",
    "X1_df.loc[:,:] = scaler.transform(X1_df)\n",
    "K1_df.loc[:,:] = scaler.transform(K1_df)\n",
    "scaler = MinMaxScaler().fit(X2_df)\n",
    "X2_df.loc[:,:] = scaler.transform(X2_df)\n",
    "K2_df.loc[:,:] = scaler.transform(K2_df)\n",
    "scaler = MinMaxScaler().fit(X3_df)\n",
    "X3_df.loc[:,:] = scaler.transform(X3_df)\n",
    "K3_df.loc[:,:] = scaler.transform(K3_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "[False False  True  True  True  True  True  True  True  True  True  True\n",
    "  True  True  True  True  True  True  True  True  True  True  True False\n",
    "  True  True  True  True  True  True False  True  True  True  True  True\n",
    "  True  True  True  True  True  True False  True  True  True  True  True\n",
    " False  True  True  True  True  True  True  True  True  True  True  True\n",
    " False  True  True  True  True  True  True  True  True  True  True  True\n",
    "  True  True  True  True  True  True  True False  True  True  True  True\n",
    "  True  True  True  True  True  True  True  True  True  True  True  True\n",
    "  True  True  True  True False  True False  True  True  True  True  True\n",
    "  True  True  True  True  True  True  True  True  True  True  True  True\n",
    "  True False False False  True  True False  True  True  True  True  True\n",
    "  True  True  True  True  True  True  True  True  True  True  True  True\n",
    "  True False  True  True  True  True  True  True  True  True  True False\n",
    "  True False False  True  True False  True  True  True False  True  True\n",
    "  True  True  True  True  True  True  True False  True  True False  True\n",
    "  True  True  True  True  True  True  True  True  True  True  True  True\n",
    "  True  True  True  True False False  True  True  True  True  True  True\n",
    "  True  True  True  True  True False  True  True  True  True  True  True\n",
    "  True  True False  True  True  True  True  True False  True  True  True\n",
    " False  True  True  True  True  True  True False False  True  True  True\n",
    "  True  True  True False  True  True  True  True  True  True  True  True\n",
    "  True False False False False False False False  True  True  True  True\n",
    "  True  True False  True  True  True  True  True  True  True  True  True\n",
    " False  True  True False  True  True  True  True  True  True  True  True\n",
    "  True False  True  True  True  True False  True False  True  True  True\n",
    "  True  True  True]\n",
    "[32 20  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 21\n",
    "  1  1  1  1  1  1  7  1  1  1  1  1  1  1  1  1  1  1 44  1  1  1  1  1\n",
    " 31  1  1  1  1  1  1  1  1  1  1  1 18  1  1  1  1  1  1  1  1  1  1  1\n",
    "  1  1  1  1  1  1  1 27  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
    "  1  1  1  1  2  1 24  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
    "  1 19 40 34  1  1 36  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
    "  1 41  1  1  1  1  1  1  1  1  1 26  1 10 11  1  1  5  1  1  1 39  1  1\n",
    "  1  1  1  1  1  1  1 30  1  1 17  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
    "  1  1  1  1  3  4  1  1  1  1  1  1  1  1  1  1  1 38  1  1  1  1  1  1\n",
    "  1  1 13  1  1  1  1  1 29  1  1  1 43  1  1  1  1  1  1 25 33  1  1  1\n",
    "  1  1  1 35  1  1  1  1  1  1  1  1  1 23 22  9  8 42 16 15  1  1  1  1\n",
    "  1  1 14  1  1  1  1  1  1  1  1  1  6  1  1 28  1  1  1  1  1  1  1  1\n",
    "  1 45  1  1  1  1 37  1 12  1  1  1  1  1  1]\n",
    "'''\n",
    "\n",
    "\n",
    "#print('\\n\\n\\n\\nBreve avaliação das primeiras amostras')\n",
    "#display(X_train_df.head(10))\n",
    "\n",
    "#print('Avaliação do descritivo do dataset que permite ter uma ideia mais realista dos dados')\n",
    "#display(X_train_df.describe())\n",
    "\n",
    "utils.beep(1, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise exploratória\n",
    "\n",
    "Nesta seção são exibidas informações do resultado após os dados serem pré-processados e os atributos selecionados. Dentre as ferramentas para análise exploratória que serão utilizados estão:\n",
    "\n",
    "* Descritivo resumido da base.\n",
    "* Análises de covariância e correlação.\n",
    "* Matriz de disperação entre todos os atributos selecionados. \n",
    "* Diagramas de violino para visualização dos quartis e outliers como uma variação aos diagramas de caixa.\n",
    "* Gráfico de dispersão com a dimensionalidade reduzida a somente 2 atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scripts import analise_exploratoria\n",
    "\n",
    "exploratory_analisys = False\n",
    "\n",
    "if exploratory_analisys == True:\n",
    "    # Análise do balanceamento das classes\n",
    "    print('Análise e visualização dos dados:')\n",
    "    ax = sns.countplot(x='classe', data=pd.concat([X_train_df,y_train_df], axis=1), label=\"Contagem\")\n",
    "\n",
    "    N,P = y_train_df.classe.value_counts()\n",
    "    print('Número de posts comuns:', N)\n",
    "    print('Número de posts phishing:', P)\n",
    "    plt.show()\n",
    "\n",
    "    # Análise da matriz scatter pra que entendamos a relação entre os atributos\n",
    "    print('Análise da matriz de dispersão')\n",
    "    sns.pairplot(input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)], hue='classe', height=3.5);\n",
    "    plt.show()\n",
    "\n",
    "    # matrizes de covariancia e correlação\n",
    "    print('Análise das matrizes de covariância ')\n",
    "    df_covariance = X_train_df.iloc[:,:-1].cov()\n",
    "    df_correlation = X_train_df.iloc[:,:-1].corr()\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20, 8))\n",
    "    plt.title('Covariância')\n",
    "    sns.heatmap(df_covariance, annot=True, xticklabels=df_correlation.columns, \n",
    "                yticklabels=df_correlation.columns, ax=ax1)\n",
    "    plt.title('Correlação')\n",
    "    sns.heatmap(df_correlation, annot=True, xticklabels=df_correlation.columns, \n",
    "                yticklabels=df_correlation.columns, ax=ax2)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Diagramas de caixa\n",
    "    plt.figure(figsize=(20,10))\n",
    "    data = pd.melt(pd.concat([X_train_df,y_train_df], axis=1),\n",
    "                   id_vars=\"classe\", var_name=\"features\", value_name='value')\n",
    "    sns.boxplot(x='features', y='value', hue='classe', data=data)\n",
    "    plt.show()\n",
    "\n",
    "    # Diagramas de violino\n",
    "    plt.figure(figsize=(20,10))\n",
    "    sns.violinplot(x='features', y='value', hue='classe', data=data, split=True, inner=\"quartile\")\n",
    "    plt.show()\n",
    "\n",
    "    # Separação de atributos e classe para \n",
    "    analise_exploratoria.printPCA(X_train_df.values,y_train_df.values.T[0])    \n",
    "\n",
    "    analise_exploratoria.printJointPlot(X_train_df,y_train_df)    \n",
    "\n",
    "utils.beep(1, 600)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Preparo dos dados e experimentos para encontrar os melhores hiperparâmetros\n",
    "\n",
    "Nesta seção os dados são separados em duas partes: treino e testes. Esta estratégia foi utilizada para posterior comparação com a validação de modelos utilizandos K-folds.\n",
    "Para encontrar os melhores hiperparâmetros foi utilizada a classe GridSearchCV e devido o seu alto custo computacional e não existência da necessidade de executá-las sempre, a sua chamada é condicionada às variáveis booleanas evaluate_svm_hiperparameters, evaluate_rfc_hiperparameters e evaluate_lrc_hiperparameters serem verdadeiras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removendo amostras outliers - Removido pois estava piorando os resultados\n"
     ]
    }
   ],
   "source": [
    "from scripts import preprocessamento\n",
    "from scripts import experimentos\n",
    "from scripts import utils\n",
    "\n",
    "X1_train = X1_train_df.values\n",
    "y1_train = y1_train_df.values.T[0]\n",
    "X1_test = X1_test_df.values\n",
    "y1_test = y1_test_df.values.T[0]\n",
    "X2_train = X2_train_df.values\n",
    "y2_train = y2_train_df.values.T[0]\n",
    "X2_test = X2_test_df.values\n",
    "y2_test = y2_test_df.values.T[0]\n",
    "X3_train = X3_train_df.values\n",
    "y3_train = y3_train_df.values.T[0]\n",
    "X3_test = X3_test_df.values\n",
    "y3_test = y3_test_df.values.T[0]\n",
    "\n",
    "X1 = X1_df.values\n",
    "X2 = X2_df.values\n",
    "X3 = X3_df.values\n",
    "y = y_df.values.T[0]\n",
    "\n",
    "# Utilize as flags abaixo somente para avaliação dos hiperparâmetros pois elas demoram muito pra serem executadas\n",
    "evaluate_svm_hiperparameters = False # Busca os melhores parâmetros para as máquinas de vetores de suporte\n",
    "evaluate_rfc_hiperparameters = False # Busca os melhores parâmetros para as florestas aleatórias\n",
    "evaluate_knn_hiperparameters = False # Busca os melhores parâmetros para o KNN\n",
    "evaluate_mlp_hiperparameters = False # Busca os melhores parâmetros para o KNN\n",
    "\n",
    "print('Removendo amostras outliers - Removido pois estava piorando os resultados')\n",
    "#X_train, y_train = preprocessamento.remove_outliers(X_train, y_train)\n",
    "#X,y = preprocessamento.remove_outliers(X, y)\n",
    "\n",
    "scores = [ \n",
    "        #    'balanced_accuracy', \n",
    "        #    'f1', \n",
    "            'roc_auc'\n",
    "        ]\n",
    "\n",
    "if evaluate_svm_hiperparameters == True:\n",
    "    experimentos.find_best_svm(X,y, scores)\n",
    "    \n",
    "if evaluate_rfc_hiperparameters == True:    \n",
    "    experimentos.find_best_rfc(X, y, scores)\n",
    "        \n",
    "if evaluate_knn_hiperparameters == True:\n",
    "    experimentos.find_best_knn(X, y, scores)\n",
    "\n",
    "if evaluate_mlp_hiperparameters == True:\n",
    "    experimentos.find_best_mlp(X, y, scores)\n",
    "    \n",
    "\n",
    "utils.beep(1, 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Experimento\n",
    "\n",
    "Nesta seção, o experimento deve ser conduzido, utilizando os protocolos experimentais ensinados no curso e executando os métodos inteligentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balaceamento antes da SOBREamostragem 3230 218\n",
      "Balaceamento após da SOBREamostragem 3230 654\n",
      "Balaceamento antes da SOBREamostragem 4038 272\n",
      "Balaceamento após da SOBREamostragem 4038 816\n",
      "Balaceamento antes da SOBREamostragem 3230 218\n",
      "Balaceamento após da SOBREamostragem 3230 654\n",
      "Balaceamento antes da SOBREamostragem 4038 272\n",
      "Balaceamento após da SOBREamostragem 4038 816\n",
      "Balaceamento antes da SOBREamostragem 3230 218\n",
      "Balaceamento após da SOBREamostragem 3230 654\n",
      "Balaceamento antes da SOBREamostragem 4038 272\n",
      "Balaceamento após da SOBREamostragem 4038 816\n",
      "MLP7------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Importa os classificadores usados nesse trabalho\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from scripts import experimentos\n",
    "from scripts import preprocessamento\n",
    "\n",
    "collect_more_samples = False\n",
    "collect_more_samples_again = False\n",
    "\n",
    "X1_train_bal, y1_train_bal = X1_train, y1_train\n",
    "X2_train_bal, y2_train_bal = X2_train, y2_train\n",
    "X3_train_bal, y3_train_bal = X3_train, y3_train\n",
    "\n",
    "scoring=['roc_auc', 'f1', 'f1_micro', 'f1_macro', 'f1_weighted', 'accuracy', 'balanced_accuracy', 'precision', 'recall']\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "model_list = [\n",
    "#    ['SVM poly', svm.SVC(kernel='poly', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "#    ['SVM linear', svm.SVC(kernel='linear', C=100, class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "#    ['SVM rbf', svm.SVC(kernel='rbf', C=1000, gamma=0.0001, class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "#    ['SVM rbf RA 50', svm.SVC(kernel='rbf', C=50, gamma='scale', class_weight='balanced', probability=True,random_state=1), 0],\n",
    "#    ['SVM rbf RA 20', svm.SVC(kernel='rbf', C=20, gamma='scale', class_weight='balanced', probability=True,random_state=1), 0],\n",
    "    #['SVM rbf RA', svm.SVC(kernel='rbf', C=10, gamma='scale', class_weight='balanced', probability=True,random_state=1), 0],\n",
    "#    ['SVM Optimal', svm.SVC(kernel='rbf', C=1, gamma='scale', class_weight='balanced', probability=True,random_state=1), 0],\n",
    "#    ['Random Forest', RandomForestClassifier(max_depth=2, class_weight='balanced', random_state=1), 0],\n",
    "#    ['Random Forest Optimal', RandomForestClassifier(criterion='gini', max_depth=6, max_features='auto', n_estimators=500, class_weight='balanced', random_state=1), 0],\n",
    "#    ['Logistic Regression', LogisticRegression(random_state=1, class_weight='balanced', max_iter=15000), 0],\n",
    "#    ['Multinomial NB', MultinomialNB(), 0],\n",
    "#     ['MLP', MLPClassifier( alpha=1e-5, hidden_layer_sizes=(600,), random_state=1, max_iter=5000, ), 0],\n",
    "     ['MLP7', MLPClassifier( alpha=1e-5, hidden_layer_sizes=(700,), random_state=1, max_iter=5000, ), 0],\n",
    "#    ['MLP rec3', MLPClassifier( alpha=1e-5, hidden_layer_sizes=(600,40,), random_state=1, max_iter=5000, ), 0],\n",
    "    #['MLP Optimal', MLPClassifier( alpha=1e-5, hidden_layer_sizes=(600,), random_state=1, max_iter=5000, ), 0],\n",
    "#    ['KNN', KNeighborsClassifier(n_neighbors=1, leaf_size=4, weights='distance'), 0],  \n",
    "#    ['KNN Optimal', KNeighborsClassifier(algorithm='auto', n_neighbors=29, leaf_size=1, weights='distance'), 0],\n",
    "#    ['AdaBoost', AdaBoostClassifier(n_estimators=100, random_state=0), 0],\n",
    "#    ['GradBoost', GradientBoostingClassifier(random_state=0), 0],\n",
    "]\n",
    "\n",
    "# Pré balanceamento dos dados utilizando a técnica de oversampling\n",
    "X1_train_bal, y1_train_bal = preprocessamento.oversample(X1_train_bal, y1_train_bal, times=3)\n",
    "X1_bal, y_bal = preprocessamento.oversample(X1, y, times=3)\n",
    "X2_train_bal, y2_train_bal = preprocessamento.oversample(X2_train_bal, y2_train_bal, times=3)\n",
    "X2_bal, y_bal = preprocessamento.oversample(X2, y, times=3)\n",
    "X3_train_bal, y3_train_bal = preprocessamento.oversample(X3_train_bal, y3_train_bal, times=3)\n",
    "X3_bal, y_bal = preprocessamento.oversample(X3, y, times=3)\n",
    "\n",
    "# População dos dados não classificados com o melhor classificador encontrado com os resultados mais confiáveis\n",
    "if collect_more_samples == True:\n",
    "    semi_model = svm.SVC(kernel='poly', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1)\n",
    "    \n",
    "    X_semi, y_semi, proba_mask = preprocessamento.add_samples(semi_model, X_bal, y_bal, samples_df.values)\n",
    "\n",
    "    print('Número de amostras acrescentadas ao dataset de treinamento:', X_semi.shape[0], \n",
    "                                                                          np.sum(y_semi==-1), np.sum(y_semi==1))\n",
    "    \n",
    "    pos_mask = y_semi==1\n",
    "    X_semi_positives = X_semi[pos_mask]\n",
    "    y_semi_positives = y_semi[pos_mask]\n",
    "    \n",
    "    X_semi_positives = X_semi_positives\n",
    "    y_semi_positives = y_semi_positives\n",
    "    \n",
    "    \n",
    "    X_train_bal = np.concatenate([X_train_bal, X_semi_positives], axis=0)\n",
    "    y_train_bal = np.concatenate([y_train_bal, y_semi_positives], axis=0)\n",
    "    X_bal = np.concatenate([X_bal, X_semi_positives], axis=0)\n",
    "    y_bal = np.concatenate([y_bal, y_semi_positives], axis=0)\n",
    "\n",
    "if collect_more_samples_again == True:\n",
    "    X_train_bal, y_train_bal = preprocessamento.oversample(X_train_bal, y_train_bal)\n",
    "    X_bal, y_bal = preprocessamento.oversample(X_bal, y_bal)\n",
    "\n",
    "    # População dos dados não classificados com o melhor classificador encontrado com os resultados mais confiáveis\n",
    "    samples = samples[~proba_mask]\n",
    "    X_semi, y_semi, proba_mask = preprocessamento.add_samples(semi_model, X_train_bal, y_train_bal, samples)\n",
    "    print('Número de amostras acrescentadas ao dataset de treinamento:', X_semi.shape[0], \n",
    "                                                                          np.sum(y_semi==-1), np.sum(y_semi==1))\n",
    "    X_train_bal = np.concatenate([X_train_bal, X_semi], axis=0)\n",
    "    y_train_bal = np.concatenate([y_train_bal, y_semi], axis=0)\n",
    "    X_bal = np.concatenate([X_bal, X_semi], axis=0)\n",
    "    y_bal = np.concatenate([y_bal, y_semi], axis=0)\n",
    "\n",
    "#print(X_bal.shape, X_train_bal.shape, np.sum(y_bal==-1),np.sum(y_bal==1), np.sum(y_train_bal==-1),np.sum(y_train_bal==1))\n",
    "\n",
    "for model in model_list:\n",
    "    print(model[0] + '------------------------------------------------------------------------------------')\n",
    "    #scores = experimentos.evaluate_model(model[1], X_train_bal, y_train_bal, X_test, y_test)\n",
    "    scores = experimentos.cross_classifier(model[1], X1_train_bal, y1_train_bal, X1_test, y1_test,\n",
    "                                                     X2_train_bal, y2_train_bal, X2_test, y2_test,\n",
    "                                                     X3_train_bal, y3_train_bal, X3_test, y3_test,\n",
    "                                          )\n",
    "    for score in scores:\n",
    "        results.loc[model[0], score] = np.mean(scores[score])\n",
    "    \n",
    "\n",
    "\n",
    "utils.beep(1, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise dos Resultados\n",
    "\n",
    "Nesta seção, os resultados devem ser exibidos e comparados, através de tabelas e gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col0,#T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col1,#T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col2,#T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col3,#T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col4,#T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col5,#T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col6,#T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col7,#T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col8{\n",
       "            background-color:  yellow;\n",
       "        }</style><table id=\"T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >bal_acc_test</th>        <th class=\"col_heading level0 col1\" >f1_weighted</th>        <th class=\"col_heading level0 col2\" >f1_micro</th>        <th class=\"col_heading level0 col3\" >precision</th>        <th class=\"col_heading level0 col4\" >recall</th>        <th class=\"col_heading level0 col5\" >mcc</th>        <th class=\"col_heading level0 col6\" >roc_auc</th>        <th class=\"col_heading level0 col7\" >TP</th>        <th class=\"col_heading level0 col8\" >TN</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0level0_row0\" class=\"row_heading level0 row0\" >MLP7</th>\n",
       "                        <td id=\"T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col0\" class=\"data row0 col0\" >0.6432</td>\n",
       "                        <td id=\"T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col1\" class=\"data row0 col1\" >0.9255</td>\n",
       "                        <td id=\"T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col2\" class=\"data row0 col2\" >0.9304</td>\n",
       "                        <td id=\"T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col3\" class=\"data row0 col3\" >0.4250</td>\n",
       "                        <td id=\"T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col4\" class=\"data row0 col4\" >0.3148</td>\n",
       "                        <td id=\"T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col5\" class=\"data row0 col5\" >0.3299</td>\n",
       "                        <td id=\"T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col6\" class=\"data row0 col6\" >0.6573</td>\n",
       "                        <td id=\"T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col7\" class=\"data row0 col7\" >785.0000</td>\n",
       "                        <td id=\"T_11c599a4_4edc_11eb_bfaf_2d33c8526cf0row0_col8\" class=\"data row0 col8\" >17.0000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7faf2af75bb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scripts import analise_resultados\n",
    "\n",
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    \n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "pd.set_option('precision', 4)\n",
    "results_highlited = results.style.apply(highlight_max)\n",
    "display(results_highlited)\n",
    "\n",
    "#best_model.fit(X,y)\n",
    "#y_pred = best_model.predict(X_test)\n",
    "#analise_resultados.print_rocauc_curve(y_test, y_pred)\n",
    "#analise_resultados.plot_decision_boundaries(X_train, y_train, svm.SVC, kernel='poly', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1)\n",
    "\n",
    "\n",
    "utils.beep(3, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Criação do arquivo de sumissão no Kaggle\n",
    "\n",
    "Na etapa final, o arquivo submission.csv é criado para ser enviado ao Kaggle de acordo com os padrões pré-definidos pela proposta do desafio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando arquivo de submissão para o modelo: MLP7\n",
      "Imprimindo arquivo submission.csv ...\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "submit_preference_score = 'MLP7'\n",
    "\n",
    "for model in model_list:\n",
    "    if model[0]==submit_preference_score:\n",
    "        best_model = model[1]\n",
    "        \n",
    "# Montagem do dataset de teste para envio para o Kaggle\n",
    "\n",
    "\n",
    "print('Criando arquivo de submissão para o modelo: ' + submit_preference_score)    \n",
    "    \n",
    "print('Imprimindo arquivo submission.csv ...')\n",
    "clf1= best_model.fit(X1_bal, y_bal)\n",
    "y1_proba = clf1.predict_proba(K1_df)\n",
    "clf2= best_model.fit(X2_bal, y_bal)\n",
    "y2_proba = clf2.predict_proba(K2_df)\n",
    "clf3= best_model.fit(X3_bal, y_bal)\n",
    "y3_proba = clf3.predict_proba(K3_df)\n",
    "\n",
    "y_proba = np.empty(y1_proba.shape)\n",
    "\n",
    "\n",
    "for i in range(y1_proba.shape[0]):\n",
    "    if( (abs(y1_proba[i][0]-y1_proba[i][1]) >= abs(y2_proba[i][0]-y2_proba[i][1])) and (abs(y1_proba[i][0]-y1_proba[i][1]) >= abs(y3_proba[i][0]-y3_proba[i][1])) ):\n",
    "        y_proba[i] = y1_proba[i]\n",
    "    if( (abs(y2_proba[i][0]-y2_proba[i][1]) >= abs(y1_proba[i][0]-y1_proba[i][1])) and (abs(y2_proba[i][0]-y2_proba[i][1]) >= abs(y3_proba[i][0]-y3_proba[i][1])) ):\n",
    "        y_proba[i] = y2_proba[i]\n",
    "    if( (abs(y3_proba[i][0]-y3_proba[i][1]) >= abs(y2_proba[i][0]-y2_proba[i][1])) and (abs(y3_proba[i][0]-y3_proba[i][1]) >= abs(y1_proba[i][0]-y1_proba[i][1])) ):\n",
    "        y_proba[i] = y3_proba[i]\n",
    "\n",
    "\n",
    "y_pred_submission = y_proba[:,1]\n",
    "\n",
    "\n",
    "result = np.zeros((K1_df.shape[0],2))\n",
    "for i in range(K1_df.shape[0]):\n",
    "    result[i][0] = test_dataset.iloc[:,:].values.T[0][i]\n",
    "    result[i][1] = y_pred_submission[i]\n",
    "resultdf = pd.DataFrame(data=result, columns=[\"Id\", \"Predicted\"])\n",
    "resultdf['Id'] = resultdf['Id'].astype(int)\n",
    "resultdf['Predicted'] = resultdf['Predicted'].round(decimals=5)\n",
    "resultdf.to_csv('submission.csv', index=False, float_format='%.5f')\n",
    "print('####################################################################################################')\n",
    "\n",
    "utils.beep(5, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
