{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"figs/LogoUFSCar.jpg\" alt=\"Logo UFScar\" width=\"110\" align=\"left\"/>  <br/> <center>Universidade Federal de São Carlos (UFSCar)<br/><font size=\"4\"> Departamento de Computação, campus Sorocaba</center></font>\n",
    "</p>\n",
    "\n",
    "<font size=\"4\"><center><b>Disciplina: Aprendizado de Máquina</b></center></font>\n",
    "  \n",
    "<font size=\"3\"><center>Prof. Dr. Tiago A. Almeida</center></font>\n",
    "\n",
    "## <center>Projeto Final</center>\n",
    "\n",
    "**Aluno**: Eduardo Garcia do Nascimento\n",
    "\n",
    "**RA/CPF**: 22008732800\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Carregamento dos dados\n",
    "\n",
    "Nesta seção é feita a carga dos atributos em um dataframe só, ou seja, os três datasets são lidos e concatenados para que a redução de atributos leve em conta o que existe de melhor em todos eles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A base de dados inicial combinada tem 60842 amostras com 110 atributos.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Caminho dos arquivos\n",
    "FILES_DIRECTORY = \"data\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from scripts import utils\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "                      \n",
    "    # importa o arquivo e guarda em um dataframe do Pandas\n",
    "    set1_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set1.csv'), sep=',', low_memory=False)\n",
    "    set2_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set2.csv'), sep=',', low_memory=False) \n",
    "    set3_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set3.csv'), sep=',', low_memory=False)\n",
    "    train_dataset = pd.read_csv(os.path.join(FILES_DIRECTORY, 'train.csv'), sep=',')\n",
    "    test_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'test.csv'), sep=',')\n",
    "    \n",
    "    # Renomeia colunas concatenando o setX antes de fazer o merge para identificá-las posteriormente\n",
    "    cols = set1_dataset.columns\n",
    "    for col in cols:\n",
    "        set1_dataset = set1_dataset.rename(columns={col:'set1_'+col})\n",
    "        \n",
    "    cols = set2_dataset.columns\n",
    "    for col in cols:\n",
    "        set2_dataset = set2_dataset.rename(columns={col:'set2_'+col})\n",
    "    \n",
    "    cols = set3_dataset.columns\n",
    "    for col in cols:\n",
    "        set3_dataset = set3_dataset.rename(columns={col:'set3_'+col})\n",
    "\n",
    "    # Concatena os datasets em somente um dataset único\n",
    "    frames = [ set2_dataset ]\n",
    "    input_dataset = pd.concat(frames, axis=1)\n",
    "    input_dataset['classe'] = np.nan\n",
    "    input_dataset.loc[train_dataset['Id'].values,'classe'] = train_dataset['Class'].values\n",
    "    \n",
    "    samples_mask = (input_dataset.classe!=-1)&(input_dataset.classe!=1)&(input_dataset.classe!=0)\n",
    "    samples_df = input_dataset[samples_mask].drop('classe', axis=1)\n",
    "    \n",
    "    K_df = input_dataset.loc[test_dataset.iloc[:,:].values.T[0]].copy()\n",
    "    K_df = K_df.drop('classe', axis=1)\n",
    "    \n",
    "    print('A base de dados inicial combinada tem %d amostras com %d atributos.' % (input_dataset.shape[0],\n",
    "                                                                                 input_dataset.shape[1]))\n",
    "    \n",
    "        \n",
    "   \n",
    "    utils.beep(1, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Pré-processamento e seleção de atributos\n",
    "\n",
    "Nesta seção são feitas limpezas da base de dados como:\n",
    "\n",
    "* Remoção de atributos sem variância;\n",
    "* Tratamento de outliers e dados nulos;\n",
    "* Seleção dos atributos que terão maior valor para o algoritmo de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4310, 110)\n",
      "Separando a base em treino e teste\n",
      "Atributos removidos por baixa variância: 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Breve avaliação das primeiras amostras\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set2_Id</th>\n",
       "      <th>set2_D0000</th>\n",
       "      <th>set2_D0001</th>\n",
       "      <th>set2_D0002</th>\n",
       "      <th>set2_D0003</th>\n",
       "      <th>set2_D0004</th>\n",
       "      <th>set2_D0005</th>\n",
       "      <th>set2_D0006</th>\n",
       "      <th>set2_D0007</th>\n",
       "      <th>set2_D0008</th>\n",
       "      <th>...</th>\n",
       "      <th>set2_D0096</th>\n",
       "      <th>set2_D0097</th>\n",
       "      <th>set2_D0100</th>\n",
       "      <th>set2_D0101</th>\n",
       "      <th>set2_D0102</th>\n",
       "      <th>set2_D0103</th>\n",
       "      <th>set2_D0104</th>\n",
       "      <th>set2_D0105</th>\n",
       "      <th>set2_D0106</th>\n",
       "      <th>set2_D0107</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30929</th>\n",
       "      <td>0.508180</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.387707</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.020230</td>\n",
       "      <td>0.082680</td>\n",
       "      <td>0.060302</td>\n",
       "      <td>0.150754</td>\n",
       "      <td>0.233668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.020230</td>\n",
       "      <td>0.233668</td>\n",
       "      <td>0.014034</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>2.706067e-04</td>\n",
       "      <td>0.296482</td>\n",
       "      <td>3.342501e-02</td>\n",
       "      <td>0.004307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10429</th>\n",
       "      <td>0.171103</td>\n",
       "      <td>0.005971</td>\n",
       "      <td>0.083916</td>\n",
       "      <td>0.312074</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.040708</td>\n",
       "      <td>0.070655</td>\n",
       "      <td>0.078710</td>\n",
       "      <td>0.109677</td>\n",
       "      <td>0.202581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010380</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.040708</td>\n",
       "      <td>0.202581</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>1.214453e-04</td>\n",
       "      <td>0.248677</td>\n",
       "      <td>1.509530e-02</td>\n",
       "      <td>0.010948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32114</th>\n",
       "      <td>0.527665</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>0.104895</td>\n",
       "      <td>0.277729</td>\n",
       "      <td>0.009856</td>\n",
       "      <td>0.029853</td>\n",
       "      <td>0.081539</td>\n",
       "      <td>0.129572</td>\n",
       "      <td>0.200627</td>\n",
       "      <td>0.276907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085062</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.029853</td>\n",
       "      <td>0.276907</td>\n",
       "      <td>0.015487</td>\n",
       "      <td>0.012840</td>\n",
       "      <td>1.474312e-04</td>\n",
       "      <td>0.350052</td>\n",
       "      <td>3.346390e-03</td>\n",
       "      <td>0.022254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58526</th>\n",
       "      <td>0.961951</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.055944</td>\n",
       "      <td>0.312684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077906</td>\n",
       "      <td>0.074250</td>\n",
       "      <td>0.150862</td>\n",
       "      <td>0.193966</td>\n",
       "      <td>0.340517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.077906</td>\n",
       "      <td>0.340517</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>1.767200e-22</td>\n",
       "      <td>0.435345</td>\n",
       "      <td>1.850372e-15</td>\n",
       "      <td>0.006281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31504</th>\n",
       "      <td>0.517635</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>0.097902</td>\n",
       "      <td>0.321321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.110745</td>\n",
       "      <td>0.042937</td>\n",
       "      <td>0.081518</td>\n",
       "      <td>0.162414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028141</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.162414</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>3.707365e-04</td>\n",
       "      <td>0.191039</td>\n",
       "      <td>4.428904e-04</td>\n",
       "      <td>0.012383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42433</th>\n",
       "      <td>0.697338</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.020979</td>\n",
       "      <td>0.319706</td>\n",
       "      <td>0.062397</td>\n",
       "      <td>0.055231</td>\n",
       "      <td>0.030481</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038621</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.055231</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.028292</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>3.144645e-01</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>9.503905e-03</td>\n",
       "      <td>0.001795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57237</th>\n",
       "      <td>0.940757</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.340038</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>0.021416</td>\n",
       "      <td>0.059352</td>\n",
       "      <td>0.055944</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.139860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>0.021416</td>\n",
       "      <td>0.139860</td>\n",
       "      <td>0.017999</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>1.751032e-05</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>2.377674e-02</td>\n",
       "      <td>0.001436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40967</th>\n",
       "      <td>0.673233</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.118881</td>\n",
       "      <td>0.358704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>0.049601</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>1.436238e-05</td>\n",
       "      <td>0.168067</td>\n",
       "      <td>1.415406e-01</td>\n",
       "      <td>0.000179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42371</th>\n",
       "      <td>0.696318</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.291873</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>0.037646</td>\n",
       "      <td>0.055215</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.207207</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.037646</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.013262</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>1.387204e-05</td>\n",
       "      <td>0.171285</td>\n",
       "      <td>1.282569e-02</td>\n",
       "      <td>0.002154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11415</th>\n",
       "      <td>0.187316</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.020979</td>\n",
       "      <td>0.246044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026406</td>\n",
       "      <td>0.033535</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.026406</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>4.497224e-06</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.968122e-02</td>\n",
       "      <td>0.001077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        set2_Id  set2_D0000  set2_D0001  set2_D0002  set2_D0003  set2_D0004  \\\n",
       "30929  0.508180    0.003063    0.076923    0.387707    0.003895    0.020230   \n",
       "10429  0.171103    0.005971    0.083916    0.312074    0.000373    0.040708   \n",
       "32114  0.527665    0.007376    0.104895    0.277729    0.009856    0.029853   \n",
       "58526  0.961951    0.001782    0.055944    0.312684    0.000000    0.077906   \n",
       "31504  0.517635    0.012390    0.097902    0.321321    0.000000    0.028434   \n",
       "42433  0.697338    0.000301    0.020979    0.319706    0.062397    0.055231   \n",
       "57237  0.940757    0.001096    0.013986    0.340038    0.006247    0.021416   \n",
       "40967  0.673233    0.000262    0.118881    0.358704    0.000000    0.014870   \n",
       "42371  0.696318    0.000849    0.013986    0.291873    0.003077    0.037646   \n",
       "11415  0.187316    0.000224    0.020979    0.246044    0.000000    0.026406   \n",
       "\n",
       "       set2_D0005  set2_D0006  set2_D0007  set2_D0008  ...  set2_D0096  \\\n",
       "30929    0.082680    0.060302    0.150754    0.233668  ...    0.000500   \n",
       "10429    0.070655    0.078710    0.109677    0.202581  ...    0.010380   \n",
       "32114    0.081539    0.129572    0.200627    0.276907  ...    0.085062   \n",
       "58526    0.074250    0.150862    0.193966    0.340517  ...    0.000000   \n",
       "31504    0.110745    0.042937    0.081518    0.162414  ...    0.028141   \n",
       "42433    0.030481    0.250000    0.400000    0.550000  ...    0.038621   \n",
       "57237    0.059352    0.055944    0.090909    0.139860  ...    0.000640   \n",
       "40967    0.049601    0.028571    0.057143    0.228571  ...    0.000720   \n",
       "42371    0.055215    0.108108    0.207207    0.243243  ...    0.000080   \n",
       "11415    0.033535    0.200000    0.233333    0.300000  ...    0.000040   \n",
       "\n",
       "       set2_D0097  set2_D0100  set2_D0101  set2_D0102  set2_D0103  \\\n",
       "30929    0.291667    0.020230    0.233668    0.014034    0.005137   \n",
       "10429    0.097222    0.040708    0.202581    0.013567    0.002021   \n",
       "32114    0.263889    0.029853    0.276907    0.015487    0.012840   \n",
       "58526    0.263889    0.077906    0.340517    0.013085    0.001360   \n",
       "31504    0.194444    0.028434    0.162414    0.013085    0.016543   \n",
       "42433    0.111111    0.055231    0.550000    0.028292    0.000202   \n",
       "57237    0.152778    0.021416    0.139860    0.017999    0.000767   \n",
       "40967    0.263889    0.014870    0.228571    0.013085    0.000822   \n",
       "42371    0.611111    0.037646    0.243243    0.013262    0.000874   \n",
       "11415    0.250000    0.026406    0.300000    0.013085    0.000453   \n",
       "\n",
       "         set2_D0104  set2_D0105    set2_D0106  set2_D0107  \n",
       "30929  2.706067e-04    0.296482  3.342501e-02    0.004307  \n",
       "10429  1.214453e-04    0.248677  1.509530e-02    0.010948  \n",
       "32114  1.474312e-04    0.350052  3.346390e-03    0.022254  \n",
       "58526  1.767200e-22    0.435345  1.850372e-15    0.006281  \n",
       "31504  3.707365e-04    0.191039  4.428904e-04    0.012383  \n",
       "42433  3.144645e-01    0.600000  9.503905e-03    0.001795  \n",
       "57237  1.751032e-05    0.302326  2.377674e-02    0.001436  \n",
       "40967  1.436238e-05    0.168067  1.415406e-01    0.000179  \n",
       "42371  1.387204e-05    0.171285  1.282569e-02    0.002154  \n",
       "11415  4.497224e-06    0.333333  1.968122e-02    0.001077  \n",
       "\n",
       "[10 rows x 107 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaliação do descritivo do dataset que permite ter uma ideia mais realista dos dados\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set2_Id</th>\n",
       "      <th>set2_D0000</th>\n",
       "      <th>set2_D0001</th>\n",
       "      <th>set2_D0002</th>\n",
       "      <th>set2_D0003</th>\n",
       "      <th>set2_D0004</th>\n",
       "      <th>set2_D0005</th>\n",
       "      <th>set2_D0006</th>\n",
       "      <th>set2_D0007</th>\n",
       "      <th>set2_D0008</th>\n",
       "      <th>...</th>\n",
       "      <th>set2_D0096</th>\n",
       "      <th>set2_D0097</th>\n",
       "      <th>set2_D0100</th>\n",
       "      <th>set2_D0101</th>\n",
       "      <th>set2_D0102</th>\n",
       "      <th>set2_D0103</th>\n",
       "      <th>set2_D0104</th>\n",
       "      <th>set2_D0105</th>\n",
       "      <th>set2_D0106</th>\n",
       "      <th>set2_D0107</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "      <td>3448.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.497861</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>0.042571</td>\n",
       "      <td>0.312018</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>0.058477</td>\n",
       "      <td>0.056323</td>\n",
       "      <td>0.103970</td>\n",
       "      <td>0.155510</td>\n",
       "      <td>0.249683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028586</td>\n",
       "      <td>0.184074</td>\n",
       "      <td>0.058477</td>\n",
       "      <td>0.249685</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.028958</td>\n",
       "      <td>0.321748</td>\n",
       "      <td>0.019069</td>\n",
       "      <td>0.005968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.288957</td>\n",
       "      <td>0.017590</td>\n",
       "      <td>0.039183</td>\n",
       "      <td>0.066906</td>\n",
       "      <td>0.032771</td>\n",
       "      <td>0.076887</td>\n",
       "      <td>0.031470</td>\n",
       "      <td>0.079027</td>\n",
       "      <td>0.094205</td>\n",
       "      <td>0.108015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119870</td>\n",
       "      <td>0.075871</td>\n",
       "      <td>0.076887</td>\n",
       "      <td>0.108015</td>\n",
       "      <td>0.020722</td>\n",
       "      <td>0.018014</td>\n",
       "      <td>0.087440</td>\n",
       "      <td>0.115996</td>\n",
       "      <td>0.041939</td>\n",
       "      <td>0.021970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.250547</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.274541</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.020350</td>\n",
       "      <td>0.044474</td>\n",
       "      <td>0.058944</td>\n",
       "      <td>0.101367</td>\n",
       "      <td>0.185409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.020350</td>\n",
       "      <td>0.185409</td>\n",
       "      <td>0.013150</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.003104</td>\n",
       "      <td>0.000897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.491762</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.034965</td>\n",
       "      <td>0.305504</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.037142</td>\n",
       "      <td>0.058478</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.141877</td>\n",
       "      <td>0.237183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>0.037142</td>\n",
       "      <td>0.237183</td>\n",
       "      <td>0.013609</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.310034</td>\n",
       "      <td>0.009653</td>\n",
       "      <td>0.003051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.749507</td>\n",
       "      <td>0.003134</td>\n",
       "      <td>0.062937</td>\n",
       "      <td>0.341410</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>0.064305</td>\n",
       "      <td>0.066495</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.196652</td>\n",
       "      <td>0.301471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003080</td>\n",
       "      <td>0.236111</td>\n",
       "      <td>0.064305</td>\n",
       "      <td>0.301471</td>\n",
       "      <td>0.014685</td>\n",
       "      <td>0.004205</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.021101</td>\n",
       "      <td>0.006506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           set2_Id   set2_D0000   set2_D0001   set2_D0002   set2_D0003  \\\n",
       "count  3448.000000  3448.000000  3448.000000  3448.000000  3448.000000   \n",
       "mean      0.497861     0.002827     0.042571     0.312018     0.007250   \n",
       "std       0.288957     0.017590     0.039183     0.066906     0.032771   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.250547     0.000409     0.013986     0.274541     0.000291   \n",
       "50%       0.491762     0.001562     0.034965     0.305504     0.002237   \n",
       "75%       0.749507     0.003134     0.062937     0.341410     0.006402   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "        set2_D0004   set2_D0005   set2_D0006   set2_D0007   set2_D0008  ...  \\\n",
       "count  3448.000000  3448.000000  3448.000000  3448.000000  3448.000000  ...   \n",
       "mean      0.058477     0.056323     0.103970     0.155510     0.249683  ...   \n",
       "std       0.076887     0.031470     0.079027     0.094205     0.108015  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.020350     0.044474     0.058944     0.101367     0.185409  ...   \n",
       "50%       0.037142     0.058478     0.090909     0.141877     0.237183  ...   \n",
       "75%       0.064305     0.066495     0.130435     0.196652     0.301471  ...   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
       "\n",
       "        set2_D0096   set2_D0097   set2_D0100   set2_D0101   set2_D0102  \\\n",
       "count  3448.000000  3448.000000  3448.000000  3448.000000  3448.000000   \n",
       "mean      0.028586     0.184074     0.058477     0.249685     0.015400   \n",
       "std       0.119870     0.075871     0.076887     0.108015     0.020722   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000120     0.125000     0.020350     0.185409     0.013150   \n",
       "50%       0.000500     0.180556     0.037142     0.237183     0.013609   \n",
       "75%       0.003080     0.236111     0.064305     0.301471     0.014685   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "        set2_D0103   set2_D0104   set2_D0105   set2_D0106   set2_D0107  \n",
       "count  3448.000000  3448.000000  3448.000000  3448.000000  3448.000000  \n",
       "mean      0.003663     0.028958     0.321748     0.019069     0.005968  \n",
       "std       0.018014     0.087440     0.115996     0.041939     0.021970  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000767     0.000016     0.250000     0.003104     0.000897  \n",
       "50%       0.002221     0.000097     0.310034     0.009653     0.003051  \n",
       "75%       0.004205     0.000603     0.384615     0.021101     0.006506  \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  \n",
       "\n",
       "[8 rows x 107 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X_df = input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)]\n",
    "\n",
    "print(X_df.shape)\n",
    "print('Separando a base em treino e teste')\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_df.drop('classe', axis=1), \n",
    "                                                    X_df[['classe']], \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=X_df[['classe']],\n",
    "                                                    random_state=0, \n",
    "                                                    )\n",
    "\n",
    "y_df = X_df[['classe']].copy()\n",
    "X_df = X_df.drop('classe', axis=1)\n",
    "\n",
    "# Remove os atributos que são constantes e não oferecem nenhum valor aos algoritmos de classificação\n",
    "variance_mask = VarianceThreshold().fit(X_train_df).get_support()\n",
    "X_train_df = X_train_df.iloc[:,variance_mask]\n",
    "X_test_df = X_test_df.iloc[:,variance_mask]\n",
    "samples_df = samples_df.iloc[:,variance_mask]\n",
    "variance_mask = np.append(variance_mask,True)\n",
    "input_dataset = input_dataset.iloc[:,variance_mask]\n",
    "print('Atributos removidos por baixa variância: %d' % np.sum(~variance_mask))\n",
    "\n",
    "variance_mask = VarianceThreshold().fit(X_df).get_support()\n",
    "X_df = X_df.iloc[:,variance_mask]\n",
    "K_df = K_df.iloc[:,variance_mask]\n",
    "\n",
    "# Tratamento de outliers e entradas nulas\n",
    "outliers_df = X_train_df.copy()\n",
    "#Q1 = outliers_df.quantile(0.25)\n",
    "#Q3 = outliers_df.quantile(0.75)\n",
    "#IQR = Q3 - Q1\n",
    "#outliers_mask = (outliers_df < (Q1 - 1.5 * IQR)) | (outliers_df > (Q3 + 1.5 * IQR))\n",
    "#print(\"Número de outliers substituídos por valores nulos:\", np.sum(np.sum(outliers_mask)))\n",
    "#outliers_df[outliers_mask] = np.nan\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imp_mean = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp_mean = imp_mean.fit(outliers_df)\n",
    "\n",
    "#imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "#imp_mean = imp_mean.fit(outliers_df)\n",
    "X_train_df.loc[:,:] = imp_mean.transform(X_train_df)\n",
    "X_test_df.loc[:,:] = imp_mean.transform(X_test_df)\n",
    "samples_df.loc[:,:] = imp_mean.transform(samples_df)\n",
    "\n",
    "\n",
    "outliers_df = X_df.copy()\n",
    "#Q1 = outliers_df.quantile(0.25)\n",
    "#Q3 = outliers_df.quantile(0.75)\n",
    "#IQR = Q3 - Q1\n",
    "#outliers_mask = (outliers_df < (Q1 - 1.5 * IQR)) | (outliers_df > (Q3 + 1.5 * IQR))\n",
    "#print(\"Número de outliers substituídos por valores nulos:\", np.sum(np.sum(outliers_mask)))\n",
    "#outliers_df[outliers_mask] = np.nan\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imp_mean = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp_mean = imp_mean.fit(outliers_df)\n",
    "\n",
    "#imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "#imp_mean = imp_mean.fit(outliers_df)\n",
    "X_df.loc[:,:] = imp_mean.transform(X_df)\n",
    "K_df.loc[:,:] = imp_mean.transform(K_df)\n",
    "\n",
    "\n",
    "\n",
    "# Normalização dos dados entre 0 e 1\n",
    "scaler = MinMaxScaler().fit(X_train_df)\n",
    "X_train_df.loc[:,:] = scaler.transform(X_train_df)\n",
    "X_test_df.loc[:,:] = scaler.transform(X_test_df)\n",
    "samples_df.loc[:,:] = scaler.transform(samples_df)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_df)\n",
    "X_df.loc[:,:] = scaler.transform(X_df)\n",
    "K_df.loc[:,:] = scaler.transform(K_df)\n",
    "\n",
    "features_mask = [ False, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True ,\n",
    "  True , True , True , True , True , True , True , True , True , True , True ,False  ,\n",
    "  True , True , True , True , True , True ,False , True , True , True , True , True  ,\n",
    "  True , True , True , True , True , True ,False , True , True , True , True , True  ,\n",
    "  False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True ,\n",
    "  False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True ,\n",
    "  True , True , True , True , True , True , True ,False , True , True , True , True  ,\n",
    "  True , True , True , True , True , True , True , True , True , True , True , True  ,\n",
    "  True , True , True , True ,False , True ,False , True , True , True , True , True  ,\n",
    "  True , True , True , True , True , True , True , True , True , True , True , True  ,\n",
    "  True ,False ,False ,False , True , True ,False , True , True , True , True , True  ,\n",
    "  True , True , True , True , True , True , True , True , True , True , True , True  ,\n",
    "  True ,False , True , True , True , True , True , True , True , True , True ,False  ,\n",
    "  True ,False ,False , True , True ,False , True , True , True ,False , True , True  ,\n",
    "  True , True , True , True , True , True , True ,False , True , True ,False , True  ,\n",
    "  True , True , True , True , True , True , True , True , True , True , True , True  ,\n",
    "  True , True , True , True ,False ,False , True , True , True , True , True , True  ,\n",
    "  True , True , True , True , True ,False , True , True , True , True , True , True  ,\n",
    "  True , True ,False , True , True , True , True , True ,False , True , True , True  ,\n",
    " False , True , True , True , True , True , True ,False ,False , True , True , True  ,\n",
    "  True , True , True ,False , True , True , True , True , True , True , True , True  ,\n",
    "  True ,False ,False ,False ,False ,False ,False ,False , True , True , True , True  ,\n",
    "  True , True ,False , True , True , True , True , True , True , True , True , True  ,\n",
    " False , True , True ,False , True , True , True , True , True , True , True , True  ,\n",
    "  True ,False , True , True , True , True ,False , True ,False , True , True , True  ,\n",
    "  True , True , True]\n",
    "\n",
    "\n",
    "# Seleciona os melhores atritubos para treinametno do algoritmo de classificação\n",
    "#print('Selecionando melhores features....................................')\n",
    "#selector = SelectKBest(f_classif, k=89).fit(\n",
    "#    X_train_df.values,\n",
    "#    y_train_df.values)\n",
    "#features_mask = selector.get_support()\n",
    "#X_train_df = X_train_df.iloc[:,features_mask]\n",
    "#X_test_df = X_test_df.iloc[:,features_mask]\n",
    "#samples_df = samples_df.iloc[:,features_mask]\n",
    "#features_mask = np.append(features_mask, True)\n",
    "#\n",
    "#selector = SelectKBest(f_classif, k=89).fit(\n",
    "#    X_df.values,\n",
    "#    y_df.values.T[0])\n",
    "#features_mask = selector.get_support()\n",
    "#X_df = X_df.iloc[:,features_mask]\n",
    "#K_df = K_df.iloc[:,features_mask]\n",
    "\n",
    "\n",
    "\n",
    "#estimator = svm.SVC(kernel=\"linear\")\n",
    "#selector = RFECV(estimator, step=1, cv=5, verbose=10)\n",
    "#selector = selector.fit(X_df.values, y_df.values.T[0])\n",
    "#print(selector.support_)\n",
    "#print(selector.ranking_)\n",
    "\n",
    "'''\n",
    "[False False  True  True  True  True  True  True  True  True  True  True\n",
    "  True  True  True  True  True  True  True  True  True  True  True False\n",
    "  True  True  True  True  True  True False  True  True  True  True  True\n",
    "  True  True  True  True  True  True False  True  True  True  True  True\n",
    " False  True  True  True  True  True  True  True  True  True  True  True\n",
    " False  True  True  True  True  True  True  True  True  True  True  True\n",
    "  True  True  True  True  True  True  True False  True  True  True  True\n",
    "  True  True  True  True  True  True  True  True  True  True  True  True\n",
    "  True  True  True  True False  True False  True  True  True  True  True\n",
    "  True  True  True  True  True  True  True  True  True  True  True  True\n",
    "  True False False False  True  True False  True  True  True  True  True\n",
    "  True  True  True  True  True  True  True  True  True  True  True  True\n",
    "  True False  True  True  True  True  True  True  True  True  True False\n",
    "  True False False  True  True False  True  True  True False  True  True\n",
    "  True  True  True  True  True  True  True False  True  True False  True\n",
    "  True  True  True  True  True  True  True  True  True  True  True  True\n",
    "  True  True  True  True False False  True  True  True  True  True  True\n",
    "  True  True  True  True  True False  True  True  True  True  True  True\n",
    "  True  True False  True  True  True  True  True False  True  True  True\n",
    " False  True  True  True  True  True  True False False  True  True  True\n",
    "  True  True  True False  True  True  True  True  True  True  True  True\n",
    "  True False False False False False False False  True  True  True  True\n",
    "  True  True False  True  True  True  True  True  True  True  True  True\n",
    " False  True  True False  True  True  True  True  True  True  True  True\n",
    "  True False  True  True  True  True False  True False  True  True  True\n",
    "  True  True  True]\n",
    "[32 20  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 21\n",
    "  1  1  1  1  1  1  7  1  1  1  1  1  1  1  1  1  1  1 44  1  1  1  1  1\n",
    " 31  1  1  1  1  1  1  1  1  1  1  1 18  1  1  1  1  1  1  1  1  1  1  1\n",
    "  1  1  1  1  1  1  1 27  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
    "  1  1  1  1  2  1 24  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
    "  1 19 40 34  1  1 36  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
    "  1 41  1  1  1  1  1  1  1  1  1 26  1 10 11  1  1  5  1  1  1 39  1  1\n",
    "  1  1  1  1  1  1  1 30  1  1 17  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
    "  1  1  1  1  3  4  1  1  1  1  1  1  1  1  1  1  1 38  1  1  1  1  1  1\n",
    "  1  1 13  1  1  1  1  1 29  1  1  1 43  1  1  1  1  1  1 25 33  1  1  1\n",
    "  1  1  1 35  1  1  1  1  1  1  1  1  1 23 22  9  8 42 16 15  1  1  1  1\n",
    "  1  1 14  1  1  1  1  1  1  1  1  1  6  1  1 28  1  1  1  1  1  1  1  1\n",
    "  1 45  1  1  1  1 37  1 12  1  1  1  1  1  1]\n",
    "'''\n",
    "\n",
    "\n",
    "print('\\n\\n\\n\\nBreve avaliação das primeiras amostras')\n",
    "display(X_train_df.head(10))\n",
    "\n",
    "print('Avaliação do descritivo do dataset que permite ter uma ideia mais realista dos dados')\n",
    "display(X_train_df.describe())\n",
    "\n",
    "utils.beep(1, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise exploratória\n",
    "\n",
    "Nesta seção são exibidas informações do resultado após os dados serem pré-processados e os atributos selecionados. Dentre as ferramentas para análise exploratória que serão utilizados estão:\n",
    "\n",
    "* Descritivo resumido da base.\n",
    "* Análises de covariância e correlação.\n",
    "* Matriz de disperação entre todos os atributos selecionados. \n",
    "* Diagramas de violino para visualização dos quartis e outliers como uma variação aos diagramas de caixa.\n",
    "* Gráfico de dispersão com a dimensionalidade reduzida a somente 2 atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scripts import analise_exploratoria\n",
    "\n",
    "exploratory_analisys = False\n",
    "\n",
    "if exploratory_analisys == True:\n",
    "    # Análise do balanceamento das classes\n",
    "    print('Análise e visualização dos dados:')\n",
    "    ax = sns.countplot(x='classe', data=pd.concat([X_train_df,y_train_df], axis=1), label=\"Contagem\")\n",
    "\n",
    "    N,P = y_train_df.classe.value_counts()\n",
    "    print('Número de posts comuns:', N)\n",
    "    print('Número de posts phishing:', P)\n",
    "    plt.show()\n",
    "\n",
    "    # Análise da matriz scatter pra que entendamos a relação entre os atributos\n",
    "    print('Análise da matriz de dispersão')\n",
    "    sns.pairplot(input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)], hue='classe', height=3.5);\n",
    "    plt.show()\n",
    "\n",
    "    # matrizes de covariancia e correlação\n",
    "    print('Análise das matrizes de covariância ')\n",
    "    df_covariance = X_train_df.iloc[:,:-1].cov()\n",
    "    df_correlation = X_train_df.iloc[:,:-1].corr()\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20, 8))\n",
    "    plt.title('Covariância')\n",
    "    sns.heatmap(df_covariance, annot=True, xticklabels=df_correlation.columns, \n",
    "                yticklabels=df_correlation.columns, ax=ax1)\n",
    "    plt.title('Correlação')\n",
    "    sns.heatmap(df_correlation, annot=True, xticklabels=df_correlation.columns, \n",
    "                yticklabels=df_correlation.columns, ax=ax2)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Diagramas de caixa\n",
    "    plt.figure(figsize=(20,10))\n",
    "    data = pd.melt(pd.concat([X_train_df,y_train_df], axis=1),\n",
    "                   id_vars=\"classe\", var_name=\"features\", value_name='value')\n",
    "    sns.boxplot(x='features', y='value', hue='classe', data=data)\n",
    "    plt.show()\n",
    "\n",
    "    # Diagramas de violino\n",
    "    plt.figure(figsize=(20,10))\n",
    "    sns.violinplot(x='features', y='value', hue='classe', data=data, split=True, inner=\"quartile\")\n",
    "    plt.show()\n",
    "\n",
    "    # Separação de atributos e classe para \n",
    "    analise_exploratoria.printPCA(X_train_df.values,y_train_df.values.T[0])    \n",
    "\n",
    "    analise_exploratoria.printJointPlot(X_train_df,y_train_df)    \n",
    "\n",
    "utils.beep(1, 600)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Preparo dos dados e experimentos para encontrar os melhores hiperparâmetros\n",
    "\n",
    "Nesta seção os dados são separados em duas partes: treino e testes. Esta estratégia foi utilizada para posterior comparação com a validação de modelos utilizandos K-folds.\n",
    "Para encontrar os melhores hiperparâmetros foi utilizada a classe GridSearchCV e devido o seu alto custo computacional e não existência da necessidade de executá-las sempre, a sua chamada é condicionada às variáveis booleanas evaluate_svm_hiperparameters, evaluate_rfc_hiperparameters e evaluate_lrc_hiperparameters serem verdadeiras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removendo amostras outliers - Removido pois estava piorando os resultados\n"
     ]
    }
   ],
   "source": [
    "from scripts import preprocessamento\n",
    "from scripts import experimentos\n",
    "from scripts import utils\n",
    "\n",
    "X_train = X_train_df.values\n",
    "y_train = y_train_df.values.T[0]\n",
    "X_test = X_test_df.values\n",
    "y_test = y_test_df.values.T[0]\n",
    "\n",
    "X = X_df.values\n",
    "y = y_df.values.T[0]\n",
    "\n",
    "# Utilize as flags abaixo somente para avaliação dos hiperparâmetros pois elas demoram muito pra serem executadas\n",
    "evaluate_svm_hiperparameters = False # Busca os melhores parâmetros para as máquinas de vetores de suporte\n",
    "evaluate_rfc_hiperparameters = False # Busca os melhores parâmetros para as florestas aleatórias\n",
    "evaluate_knn_hiperparameters = False # Busca os melhores parâmetros para o KNN\n",
    "evaluate_mlp_hiperparameters = False # Busca os melhores parâmetros para o KNN\n",
    "\n",
    "print('Removendo amostras outliers - Removido pois estava piorando os resultados')\n",
    "#X_train, y_train = preprocessamento.remove_outliers(X_train, y_train)\n",
    "#X,y = preprocessamento.remove_outliers(X, y)\n",
    "\n",
    "scores = [ \n",
    "        #    'balanced_accuracy', \n",
    "        #    'f1', \n",
    "            'roc_auc'\n",
    "        ]\n",
    "\n",
    "if evaluate_svm_hiperparameters == True:\n",
    "    experimentos.find_best_svm(X,y, scores)\n",
    "    \n",
    "if evaluate_rfc_hiperparameters == True:    \n",
    "    experimentos.find_best_rfc(X, y, scores)\n",
    "        \n",
    "if evaluate_knn_hiperparameters == True:\n",
    "    experimentos.find_best_knn(X, y, scores)\n",
    "\n",
    "if evaluate_mlp_hiperparameters == True:\n",
    "    experimentos.find_best_mlp(X, y, scores)\n",
    "    \n",
    "\n",
    "utils.beep(1, 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Experimento\n",
    "\n",
    "Nesta seção, o experimento deve ser conduzido, utilizando os protocolos experimentais ensinados no curso e executando os métodos inteligentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balaceamento antes da SOBREamostragem 3230 218\n",
      "Balaceamento após da SOBREamostragem 3230 1090\n",
      "Balaceamento antes da SOBREamostragem 4038 272\n",
      "Balaceamento após da SOBREamostragem 4038 1360\n",
      "(5398, 107) (4320, 107) 4038 1360 3230 1090\n",
      "SVM rbf------------------------------------------------------------------------------------\n",
      "SVM rbf RA 50------------------------------------------------------------------------------------\n",
      "SVM rbf RA 20------------------------------------------------------------------------------------\n",
      "SVM rbf RA------------------------------------------------------------------------------------\n",
      "SVM Optimal------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Importa os classificadores usados nesse trabalho\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from scripts import experimentos\n",
    "from scripts import preprocessamento\n",
    "\n",
    "collect_more_samples = False\n",
    "collect_more_samples_again = False\n",
    "\n",
    "X_train_bal, y_train_bal = X_train, y_train\n",
    "\n",
    "scoring=['roc_auc', 'f1', 'f1_micro', 'f1_macro', 'f1_weighted', 'accuracy', 'balanced_accuracy', 'precision', 'recall']\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "model_list = [\n",
    "#    ['SVM poly', svm.SVC(kernel='poly', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "#    ['SVM linear', svm.SVC(kernel='linear', C=100, class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "    ['SVM rbf', svm.SVC(kernel='rbf', C=1000, gamma=0.0001, class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1), 0],\n",
    "    ['SVM rbf RA 50', svm.SVC(kernel='rbf', C=50, gamma='scale', class_weight='balanced', probability=True,random_state=1), 0],\n",
    "    ['SVM rbf RA 20', svm.SVC(kernel='rbf', C=20, gamma='scale', class_weight='balanced', probability=True,random_state=1), 0],\n",
    "    ['SVM rbf RA', svm.SVC(kernel='rbf', C=10, gamma='scale', class_weight='balanced', probability=True,random_state=1), 0],\n",
    "    ['SVM Optimal', svm.SVC(kernel='rbf', C=1, gamma='scale', class_weight='balanced', probability=True,random_state=1), 0],\n",
    "#    ['Random Forest', RandomForestClassifier(max_depth=2, class_weight='balanced', random_state=1), 0],\n",
    "#    ['Random Forest Optimal', RandomForestClassifier(criterion='gini', max_depth=6, max_features='auto', n_estimators=500, class_weight='balanced', random_state=1), 0],\n",
    "#    ['Logistic Regression', LogisticRegression(random_state=1, class_weight='balanced', max_iter=15000), 0],\n",
    "#    ['Multinomial NB', MultinomialNB(), 0],\n",
    "#    ['MLP', MLPClassifier( alpha=1e-5, hidden_layer_sizes=(600,), random_state=1, max_iter=5000, ), 0],\n",
    "#    ['MLP7', MLPClassifier( alpha=1e-5, hidden_layer_sizes=(700,), random_state=1, max_iter=5000, ), 0],\n",
    "#    ['MLP rec3', MLPClassifier( alpha=1e-5, hidden_layer_sizes=(600,40,), random_state=1, max_iter=5000, ), 0],\n",
    "    #['MLP Optimal', MLPClassifier( alpha=1e-5, hidden_layer_sizes=(600,), random_state=1, max_iter=5000, ), 0],\n",
    "#    ['KNN', KNeighborsClassifier(n_neighbors=1, leaf_size=4, weights='distance'), 0],  \n",
    "#    ['KNN Optimal', KNeighborsClassifier(algorithm='auto', n_neighbors=29, leaf_size=1, weights='distance'), 0],\n",
    "#    ['AdaBoost', AdaBoostClassifier(n_estimators=100, random_state=0), 0],\n",
    "#    ['GradBoost', GradientBoostingClassifier(random_state=0), 0],\n",
    "]\n",
    "\n",
    "# Pré balanceamento dos dados utilizando a técnica de oversampling\n",
    "X_train_bal, y_train_bal = preprocessamento.oversample(X_train_bal, y_train_bal, times=5)\n",
    "X_bal, y_bal = preprocessamento.oversample(X, y, times=5)\n",
    "\n",
    "# População dos dados não classificados com o melhor classificador encontrado com os resultados mais confiáveis\n",
    "if collect_more_samples == True:\n",
    "    semi_model = svm.SVC(kernel='poly', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1)\n",
    "    \n",
    "    X_semi, y_semi, proba_mask = preprocessamento.add_samples(semi_model, X_bal, y_bal, samples_df.values)\n",
    "\n",
    "    print('Número de amostras acrescentadas ao dataset de treinamento:', X_semi.shape[0], \n",
    "                                                                          np.sum(y_semi==-1), np.sum(y_semi==1))\n",
    "    \n",
    "    pos_mask = y_semi==1\n",
    "    X_semi_positives = X_semi[pos_mask]\n",
    "    y_semi_positives = y_semi[pos_mask]\n",
    "    \n",
    "    X_semi_positives = X_semi_positives\n",
    "    y_semi_positives = y_semi_positives\n",
    "    \n",
    "    X_train_bal = np.concatenate([X_train_bal, X_semi_positives], axis=0)\n",
    "    y_train_bal = np.concatenate([y_train_bal, y_semi_positives], axis=0)\n",
    "    X_bal = np.concatenate([X_bal, X_semi_positives], axis=0)\n",
    "    y_bal = np.concatenate([y_bal, y_semi_positives], axis=0)\n",
    "\n",
    "if collect_more_samples_again == True:\n",
    "    X_train_bal, y_train_bal = preprocessamento.oversample(X_train_bal, y_train_bal)\n",
    "    X_bal, y_bal = preprocessamento.oversample(X_bal, y_bal)\n",
    "\n",
    "    # População dos dados não classificados com o melhor classificador encontrado com os resultados mais confiáveis\n",
    "    samples = samples[~proba_mask]\n",
    "    X_semi, y_semi, proba_mask = preprocessamento.add_samples(semi_model, X_train_bal, y_train_bal, samples)\n",
    "    print('Número de amostras acrescentadas ao dataset de treinamento:', X_semi.shape[0], \n",
    "                                                                          np.sum(y_semi==-1), np.sum(y_semi==1))\n",
    "    X_train_bal = np.concatenate([X_train_bal, X_semi], axis=0)\n",
    "    y_train_bal = np.concatenate([y_train_bal, y_semi], axis=0)\n",
    "    X_bal = np.concatenate([X_bal, X_semi], axis=0)\n",
    "    y_bal = np.concatenate([y_bal, y_semi], axis=0)\n",
    "\n",
    "print(X_bal.shape, X_train_bal.shape, np.sum(y_bal==-1),np.sum(y_bal==1), np.sum(y_train_bal==-1),np.sum(y_train_bal==1))\n",
    "\n",
    "for model in model_list:\n",
    "    print(model[0] + '------------------------------------------------------------------------------------')\n",
    "    scores = experimentos.evaluate_model(model[1], X_train_bal, y_train_bal, X_test, y_test)\n",
    "\n",
    "    for score in scores:\n",
    "        results.loc[model[0], score] = np.mean(scores[score])\n",
    "    \n",
    "\n",
    "\n",
    "utils.beep(1, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise dos Resultados\n",
    "\n",
    "Nesta seção, os resultados devem ser exibidos e comparados, através de tabelas e gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow0_col10,#T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow0_col11,#T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col0,#T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col2,#T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col3,#T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col4,#T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col6,#T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col8,#T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow2_col1,#T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow2_col5,#T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow2_col9,#T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow3_col5,#T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow3_col7,#T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow3_col9{\n",
       "            background-color:  yellow;\n",
       "        }</style><table id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84d\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >bal_acc_train</th>        <th class=\"col_heading level0 col1\" >bal_acc_test</th>        <th class=\"col_heading level0 col2\" >f1_weighted</th>        <th class=\"col_heading level0 col3\" >f1_micro</th>        <th class=\"col_heading level0 col4\" >precision</th>        <th class=\"col_heading level0 col5\" >recall</th>        <th class=\"col_heading level0 col6\" >mcc</th>        <th class=\"col_heading level0 col7\" >roc_auc</th>        <th class=\"col_heading level0 col8\" >TP</th>        <th class=\"col_heading level0 col9\" >TN</th>        <th class=\"col_heading level0 col10\" >train_time</th>        <th class=\"col_heading level0 col11\" >predict_time</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84dlevel0_row0\" class=\"row_heading level0 row0\" >SVM rbf</th>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow0_col0\" class=\"data row0 col0\" >0.7345</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow0_col1\" class=\"data row0 col1\" >0.6429</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow0_col2\" class=\"data row0 col2\" >0.8231</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow0_col3\" class=\"data row0 col3\" >0.7680</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow0_col4\" class=\"data row0 col4\" >0.1350</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow0_col5\" class=\"data row0 col5\" >0.5000</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow0_col6\" class=\"data row0 col6\" >0.1641</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow0_col7\" class=\"data row0 col7\" >0.6774</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow0_col8\" class=\"data row0 col8\" >635.0000</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow0_col9\" class=\"data row0 col9\" >27.0000</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow0_col10\" class=\"data row0 col10\" >11.4442</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow0_col11\" class=\"data row0 col11\" >0.3556</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84dlevel0_row1\" class=\"row_heading level0 row1\" >SVM rbf RA 50</th>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col0\" class=\"data row1 col0\" >0.9238</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col1\" class=\"data row1 col1\" >0.6807</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col2\" class=\"data row1 col2\" >0.8698</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col3\" class=\"data row1 col3\" >0.8387</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col4\" class=\"data row1 col4\" >0.1942</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col5\" class=\"data row1 col5\" >0.5000</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col6\" class=\"data row1 col6\" >0.2381</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col7\" class=\"data row1 col7\" >0.7207</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col8\" class=\"data row1 col8\" >696.0000</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col9\" class=\"data row1 col9\" >27.0000</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col10\" class=\"data row1 col10\" >6.6385</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow1_col11\" class=\"data row1 col11\" >0.1803</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84dlevel0_row2\" class=\"row_heading level0 row2\" >SVM rbf RA 20</th>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow2_col0\" class=\"data row2 col0\" >0.8993</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow2_col1\" class=\"data row2 col1\" >0.6875</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow2_col2\" class=\"data row2 col2\" >0.8575</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow2_col3\" class=\"data row2 col3\" >0.8190</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow2_col4\" class=\"data row2 col4\" >0.1812</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow2_col5\" class=\"data row2 col5\" >0.5370</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow2_col6\" class=\"data row2 col6\" >0.2337</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow2_col7\" class=\"data row2 col7\" >0.7293</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow2_col8\" class=\"data row2 col8\" >677.0000</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow2_col9\" class=\"data row2 col9\" >29.0000</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow2_col10\" class=\"data row2 col10\" >6.7552</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow2_col11\" class=\"data row2 col11\" >0.2152</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84dlevel0_row3\" class=\"row_heading level0 row3\" >SVM rbf RA</th>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow3_col0\" class=\"data row3 col0\" >0.8709</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow3_col1\" class=\"data row3 col1\" >0.6837</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow3_col2\" class=\"data row3 col2\" >0.8529</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow3_col3\" class=\"data row3 col3\" >0.8121</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow3_col4\" class=\"data row3 col4\" >0.1747</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow3_col5\" class=\"data row3 col5\" >0.5370</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow3_col6\" class=\"data row3 col6\" >0.2258</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow3_col7\" class=\"data row3 col7\" >0.7403</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow3_col8\" class=\"data row3 col8\" >671.0000</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow3_col9\" class=\"data row3 col9\" >29.0000</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow3_col10\" class=\"data row3 col10\" >7.0703</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow3_col11\" class=\"data row3 col11\" >0.2142</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84dlevel0_row4\" class=\"row_heading level0 row4\" >SVM Optimal</th>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow4_col0\" class=\"data row4 col0\" >0.8061</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow4_col1\" class=\"data row4 col1\" >0.6578</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow4_col2\" class=\"data row4 col2\" >0.8311</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow4_col3\" class=\"data row4 col3\" >0.7796</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow4_col4\" class=\"data row4 col4\" >0.1458</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow4_col5\" class=\"data row4 col5\" >0.5185</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow4_col6\" class=\"data row4 col6\" >0.1838</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow4_col7\" class=\"data row4 col7\" >0.7307</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow4_col8\" class=\"data row4 col8\" >644.0000</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow4_col9\" class=\"data row4 col9\" >28.0000</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow4_col10\" class=\"data row4 col10\" >8.9891</td>\n",
       "                        <td id=\"T_b92bec0a_4df8_11eb_b55a_55fa04d1f84drow4_col11\" class=\"data row4 col11\" >0.2714</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff36020d190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scripts import analise_resultados\n",
    "\n",
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    \n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "pd.set_option('precision', 4)\n",
    "results_highlited = results.style.apply(highlight_max)\n",
    "display(results_highlited)\n",
    "\n",
    "#best_model.fit(X,y)\n",
    "#y_pred = best_model.predict(X_test)\n",
    "#analise_resultados.print_rocauc_curve(y_test, y_pred)\n",
    "#analise_resultados.plot_decision_boundaries(X_train, y_train, svm.SVC, kernel='poly', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1)\n",
    "\n",
    "\n",
    "utils.beep(3, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Criação do arquivo de sumissão no Kaggle\n",
    "\n",
    "Na etapa final, o arquivo submission.csv é criado para ser enviado ao Kaggle de acordo com os padrões pré-definidos pela proposta do desafio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando arquivo de submissão para o modelo: SVM rbf RA\n",
      "Imprimindo arquivo submission.csv ...\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "submit_preference_score = 'SVM rbf RA'\n",
    "\n",
    "for model in model_list:\n",
    "    if model[0]==submit_preference_score:\n",
    "        best_model = model[1]\n",
    "        \n",
    "# Montagem do dataset de teste para envio para o Kaggle\n",
    "\n",
    "\n",
    "print('Criando arquivo de submissão para o modelo: ' + submit_preference_score)    \n",
    "    \n",
    "print('Imprimindo arquivo submission.csv ...')\n",
    "clf= best_model.fit(X_bal, y_bal)\n",
    "y_pred_submission = clf.predict_proba(K_df)[:,1]\n",
    "result = np.zeros((K_df.shape[0],2))\n",
    "for i in range(K_df.shape[0]):\n",
    "    result[i][0] = test_dataset.iloc[:,:].values.T[0][i]\n",
    "    result[i][1] = y_pred_submission[i]\n",
    "resultdf = pd.DataFrame(data=result, columns=[\"Id\", \"Predicted\"])\n",
    "resultdf['Id'] = resultdf['Id'].astype(int)\n",
    "resultdf['Predicted'] = resultdf['Predicted'].round(decimals=5)\n",
    "resultdf.to_csv('submission.csv', index=False, float_format='%.5f')\n",
    "print('####################################################################################################')\n",
    "\n",
    "utils.beep(5, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
