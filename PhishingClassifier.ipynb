{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"figs/LogoUFSCar.jpg\" alt=\"Logo UFScar\" width=\"110\" align=\"left\"/>  <br/> <center>Universidade Federal de São Carlos (UFSCar)<br/><font size=\"4\"> Departamento de Computação, campus Sorocaba</center></font>\n",
    "</p>\n",
    "\n",
    "<font size=\"4\"><center><b>Disciplina: Aprendizado de Máquina</b></center></font>\n",
    "  \n",
    "<font size=\"3\"><center>Prof. Dr. Tiago A. Almeida</center></font>\n",
    "\n",
    "## <center>Projeto Final</center>\n",
    "\n",
    "**Aluno**: Eduardo Garcia do Nascimento\n",
    "\n",
    "**RA/CPF**: 22008732800\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise exploratória\n",
    "\n",
    "Nesta seção, deve ser feita a leitura da base de dados e todas as análises necessárias para entendê-la melhor, tais como:\n",
    "* Significado de cada atributo\n",
    "* Medidas descritivas\n",
    "* Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Caminho dos arquivos\n",
    "FILES_DIRECTORY = \"data\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from scripts import utils\n",
    "\n",
    "if __name__ == '__main__':\n",
    "                       \n",
    "    # importa o arquivo e guarda em um dataframe do Pandas\n",
    "    set1_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set1.csv'), sep=',', low_memory=False)\n",
    "    set2_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set2.csv'), sep=',', low_memory=False) \n",
    "    set3_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'set3.csv'), sep=',', low_memory=False)\n",
    "    \n",
    "    # Renomeia colunas concatenando o setX antes de fazer o merge para identificá-las posteriormente\n",
    "    cols = set1_dataset.columns\n",
    "    for col in cols:\n",
    "        set1_dataset = set1_dataset.rename(columns={col:'set1_'+col})\n",
    "        \n",
    "    cols = set2_dataset.columns\n",
    "    for col in cols:\n",
    "        set2_dataset = set2_dataset.rename(columns={col:'set2_'+col})\n",
    "    \n",
    "    cols = set3_dataset.columns\n",
    "    for col in cols:\n",
    "        set3_dataset = set3_dataset.rename(columns={col:'set3_'+col})\n",
    "\n",
    "    # Concatena os datasets em somente um dataset único\n",
    "    frames = [ set1_dataset, set2_dataset, set3_dataset ]\n",
    "    input_dataset = pd.concat(frames, axis=1)\n",
    "    \n",
    "    # Remove os atributos que são constantes e não oferecem nenhum valor aos algoritmos de classificação\n",
    "    print('Removendo atributos com baixa variância....................................')\n",
    "    variance_mask = VarianceThreshold().fit(input_dataset).get_support()\n",
    "    input_dataset = input_dataset.iloc[:,variance_mask]\n",
    "    print('Atributos removidos por baixa variância: %d' % np.sum(~variance_mask))\n",
    "    \n",
    "    # Tratamento de outliers e entradas nulas\n",
    "    print('Removendo outliers e substituindo por valores nulos.........................')\n",
    "    df = input_dataset.copy()\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))\n",
    "    df[mask] = np.nan\n",
    "    print('Preenchendo valores nulos com a média dos atributos..........................')\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    input_dataset.loc[:,:] = imp_mean.fit_transform(input_dataset)\n",
    "    \n",
    "    # Normalização dos dados entre 0 e 1\n",
    "    input_dataset.loc[:,:] = MinMaxScaler().fit_transform(input_dataset)\n",
    "    \n",
    "    # Adiciona as classes junto ao dataset de atributos\n",
    "    train_dataset = pd.read_csv(os.path.join(FILES_DIRECTORY, 'train.csv'), sep=',')\n",
    "    input_dataset['classe'] = np.nan\n",
    "    input_dataset.loc[train_dataset['Id'].values,'classe'] = train_dataset['Class'].values\n",
    "    \n",
    "    backup_dataset = input_dataset.copy()\n",
    "    \n",
    "    mask = ((input_dataset['classe'] == -1) | (input_dataset['classe'] == 1))\n",
    "    display(input_dataset.loc[mask].head(10))\n",
    "    \n",
    "    cols = list(input_dataset.columns)\n",
    "    cols.remove('classe')\n",
    "        \n",
    "    # Seleciona os melhores atritubos para treinametno do algoritmo de classificação\n",
    "    print('Selecionando melhores features....................................')\n",
    "    selector = SelectKBest(f_classif, k=6).fit(\n",
    "        input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1), cols].values,\n",
    "        input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1),'classe'].values)\n",
    "    mask = selector.get_support()\n",
    "    mask = np.append(mask, True)\n",
    "    input_dataset = input_dataset.iloc[:,mask]\n",
    "    \n",
    "    # matriz de gráficos scatter \n",
    "    sns.pairplot(input_dataset, hue='classe', height=3.5);\n",
    "    plt.show()\n",
    "    \n",
    "    # matrizes de covariancia e correlação\n",
    "    df_covariance = input_dataset.iloc[:,:-1].cov()\n",
    "    df_correlation = input_dataset.iloc[:,:-1].corr()\n",
    "    \n",
    "    \n",
    "    # cria um mapa de cores dos valoes da covariancia\n",
    "    sns.heatmap(df_covariance, annot=True, xticklabels=df_correlation.columns, yticklabels=df_correlation.columns)\n",
    "    plt.title('Covariância')\n",
    "    plt.show()\n",
    "\n",
    "    # cria um mapa de cores dos valoes da correlação\n",
    "    sns.heatmap(df_correlation, annot=True, xticklabels=df_correlation.columns, yticklabels=df_correlation.columns)\n",
    "    plt.title('Correlação')\n",
    "    plt.show()\n",
    "    \n",
    "    display(input_dataset.info())\n",
    "    \n",
    "    print('Dados concatenados produzindo um total de %d atributos' % \n",
    "            input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)].shape[1])\n",
    "    \n",
    "    print('O número de amostras com classificação válida é: %d' % \n",
    "            input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)].shape[0])\n",
    "\n",
    "    print('Dados de treinamento carregados com sucesso!')\n",
    "\n",
    "    test_dataset  = pd.read_csv(os.path.join(FILES_DIRECTORY, 'test.csv'), sep=',')\n",
    "    K = input_dataset.loc[test_dataset.iloc[:,:].values.T[0]]\n",
    "    K = K.drop('classe', axis=1).values\n",
    "    \n",
    "    print('Análise e visualização dos dados:')\n",
    "    y = input_dataset.classe\n",
    "    ax = sns.countplot(y, label=\"Contagem\")\n",
    "    N,U,P = y.value_counts()\n",
    "    print('Número de posts comuns:', N)\n",
    "    print('Número de posts não reconhecidos:', U)\n",
    "    print('Número de posts phishing', P)\n",
    "    plt.show()\n",
    "    \n",
    "    display(input_dataset.describe())\n",
    "    \n",
    "    cl = input_dataset.columns\n",
    "    cl = cl.drop('classe')\n",
    "    \n",
    " \n",
    "    plt.figure(figsize=(20,10))\n",
    "    data = pd.melt(input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)],\n",
    "                   id_vars=\"classe\", var_name=\"features\", value_name='value')\n",
    "    sns.boxplot(x='features', y='value', hue='classe', data=data)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    sns.violinplot(x='features', y='value', hue='classe', data=data, split=True, inner=\"quartile\")\n",
    "    plt.show()\n",
    "    \n",
    "    X = input_dataset.loc[(input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)]\n",
    "    X = X.drop('classe', axis=1).values\n",
    "    y = input_dataset.loc[((input_dataset['classe'] == -1) | (input_dataset['classe'] == 1)), 'classe'].values\n",
    "    \n",
    "    utils.printPCA(X,y)    \n",
    "    \n",
    "    utils.beep(1, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Pré-processamento\n",
    "\n",
    "Nesta seção, as funções da etapa de pré-processamento dos dados devem ser implementadas e aplicadas (se necessário)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scripts import utils\n",
    "\n",
    "  \n",
    "X, y = utils.remove_outliers(X, y)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print('Separando a base em treino e teste')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=0, stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "utils.printPCA(X_train, y_train)\n",
    "utils.printPCA(X,y)\n",
    "utils.printBoxPlot(X)\n",
    "\n",
    "utils.beep(2, 600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Experimento\n",
    "\n",
    "Nesta seção, o experimento deve ser conduzido, utilizando os protocolos experimentais ensinados no curso e executando os métodos inteligentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import  plot_confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "scoring_list=['roc_auc', 'f1', 'f1_micro', 'f1_macro', 'balanced_accuracy']\n",
    "results = pd.DataFrame()\n",
    "\n",
    "# Pré balanceamento dos dados utilizando a técnica de oversampling\n",
    "X_train, y_train = utils.balance_classes(X_train, y_train)\n",
    "X_bal, y_bal = utils.balance_classes(X, y)\n",
    "\n",
    "# População dos dados não classificados com o melhor classificador encontrado com os resultados mais confiáveis\n",
    "X_semi_df = backup_dataset[(backup_dataset.classe!=-1)&(backup_dataset.classe!=1)&(backup_dataset.classe!=0)]\n",
    "X_semi_df = X_semi_df.iloc[:,mask]\n",
    "model = svm.SVC(kernel='poly', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1)\n",
    "clf = model.fit(X_train, y_train)\n",
    "y_semi = clf.predict(X_semi_df.drop('classe', axis=1).values)\n",
    "y_probas = clf.predict_proba(X_semi_df.drop('classe', axis=1).values)\n",
    "proba_mask = (y_probas[:,0] < 0.2) | (y_probas[:,0] > 0.8)\n",
    "y_semi = y_semi[proba_mask]\n",
    "X_semi_df = X_semi_df[proba_mask]\n",
    "X_semi = X_semi_df.drop('classe', axis=1).values\n",
    "\n",
    "print('Número de amostras acrescentadas ao dataset de treinamento:', X_semi.shape[0], \n",
    "                                                                      np.sum(y_semi==-1), np.sum(y_semi==1))\n",
    "X_train = np.concatenate([X_train, X_semi], axis=0)\n",
    "y_train = np.concatenate([y_train, y_semi], axis=0)\n",
    "X = np.concatenate([X, X_semi], axis=0)\n",
    "y = np.concatenate([y, y_semi], axis=0)\n",
    "\n",
    "# Configura K folds estratificados, ou seja, mantendo as mesmas proporções entre classes\n",
    "cv = StratifiedKFold(n_splits=4, random_state=1, shuffle=True)\n",
    "\n",
    "clfname = 'SVM poly'\n",
    "print(clfname + '------------------------------------------------------------------------------------')\n",
    "model = svm.SVC(kernel='poly', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1)\n",
    "utils.evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "for scoring in scoring_list:\n",
    "    scores = cross_val_score(model, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    print('Cross ' + scoring + ': %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "    results.loc[clfname, scoring] = np.mean(scores)\n",
    "    \n",
    "clfname = 'SVM linear'\n",
    "print(clfname + '------------------------------------------------------------------------------------')\n",
    "model = svm.SVC(kernel='linear', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1)\n",
    "utils.evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "for scoring in scoring_list:\n",
    "    scores = cross_val_score(model, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    print('Cross ' + scoring + ': %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "    results.loc[clfname, scoring] = np.mean(scores)\n",
    "    \n",
    "clfname = 'SVM rbf'\n",
    "print(clfname + '------------------------------------------------------------------------------------')\n",
    "model = svm.SVC(kernel='rbf', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1)\n",
    "utils.evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "for scoring in scoring_list:\n",
    "    scores = cross_val_score(model, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    print('Cross ' + scoring + ': %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "    results.loc[clfname, scoring] = np.mean(scores)\n",
    "    \n",
    "clfname = 'SVM sigmoid'\n",
    "print(clfname + '------------------------------------------------------------------------------------')\n",
    "model = svm.SVC(kernel='sigmoid', class_weight='balanced', decision_function_shape='ovr', probability=True,random_state=1)\n",
    "utils.evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "for scoring in scoring_list:\n",
    "    scores = cross_val_score(model, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    print('Cross ' + scoring + ': %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "    results.loc[clfname, scoring] = np.mean(scores)\n",
    "\n",
    "print('Printing model to submission.csv ###################################################################')\n",
    "clf= model.fit(X, y)\n",
    "y_pred_submission = clf.predict_proba(K)[:,1]\n",
    "result = np.zeros((K.shape[0],2))\n",
    "for i in range(K.shape[0]):\n",
    "    result[i][0] = test_dataset.iloc[:,:].values.T[0][i]\n",
    "    result[i][1] = y_pred_submission[i]\n",
    "resultdf = pd.DataFrame(data=result, columns=[\"Id\", \"Predicted\"])\n",
    "resultdf['Id'] = resultdf['Id'].astype(int)\n",
    "resultdf['Predicted'] = resultdf['Predicted'].round(decimals=5)\n",
    "resultdf.to_csv('submission.csv', index=False, float_format='%.5f')\n",
    "print('####################################################################################################')\n",
    "\n",
    "clfname = 'RandomForestClassifier'\n",
    "print(clfname + '------------------------------------------------------------------------------------')\n",
    "model = RandomForestClassifier(max_depth=2, class_weight='balanced', random_state=1)\n",
    "utils.evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "for scoring in scoring_list:\n",
    "    scores = cross_val_score(model, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    print('Cross ' + scoring + ': %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "    results.loc[clfname, scoring] = np.mean(scores)\n",
    "\n",
    "clfname = 'LogisticRegression'\n",
    "print(clfname + '------------------------------------------------------------------------------------')\n",
    "model = LogisticRegression(random_state=1, class_weight='balanced', max_iter=15000)\n",
    "utils.evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "for scoring in scoring_list:\n",
    "    scores = cross_val_score(model, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    print('Cross ' + scoring + ': %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "    results.loc[clfname, scoring] = np.mean(scores)\n",
    "\n",
    "X_train, y_train = utils.balance_classes(X_train, y_train)\n",
    "\n",
    "clfname = 'MLPClassifier'\n",
    "print(clfname + '------------------------------------------------------------------------------------')\n",
    "model = MLPClassifier( alpha=1e-5, hidden_layer_sizes=(600,), random_state=1, max_iter=5000, )\n",
    "utils.evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "for scoring in scoring_list:\n",
    "    scores = cross_val_score(model, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    print('Cross ' + scoring + ': %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "    results.loc[clfname, scoring] = np.mean(scores)\n",
    "\n",
    "clfname = 'MultinomialNB'\n",
    "print(clfname + '------------------------------------------------------------------------------------')\n",
    "model = MultinomialNB()\n",
    "utils.evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "for scoring in scoring_list:\n",
    "    scores = cross_val_score(model, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    print('Cross ' + scoring + ': %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "    results.loc[clfname, scoring] = np.mean(scores)\n",
    "\n",
    "clfname = 'KNeighborsClassifier'\n",
    "print(clfname + '------------------------------------------------------------------------------------')\n",
    "model = KNeighborsClassifier(weights='distance')\n",
    "utils.evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "for scoring in scoring_list:\n",
    "    scores = cross_val_score(model, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    print('Cross ' + scoring + ': %.3f (%.3f)------------------' % (np.mean(scores), np.std(scores)))\n",
    "    results.loc[clfname, scoring] = np.mean(scores)\n",
    "\n",
    "display(results)\n",
    "    \n",
    "utils.beep(3, 800)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise dos Resultados\n",
    "\n",
    "Nesta seção, os resultados devem ser exibidos e comparados, através de tabelas e gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asdfqwerqwrqwerqwer\n",
    "qeqwerqwer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
